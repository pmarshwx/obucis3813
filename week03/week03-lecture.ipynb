{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3573d1d",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "\n",
    "# **Spring 2026 &mdash; CIS 3813<br>Advanced Data Science<br>(Introduction to Machine Learning)**\n",
    "### Week 3: Linear Algebra for Data Science\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7803af9",
   "metadata": {},
   "source": [
    "**Date:** 09 February 2026\n",
    "**Time:** 6:00–9:00 PM  \n",
    "**Instructor:** Dr. Patrick T. Marsh  \n",
    "**Course Verse:** \"He has shown you, O mortal, what is good. And what does the Lord require of you? To act justly and to love mercy and to walk humbly with your God.\"  &mdash; *Micah 6:8 (NIV)*\n",
    "\n",
    "---\n",
    "## **Week 3 Learning Objectives**\n",
    "\n",
    "By the end of this lecture, you will be able to:\n",
    "\n",
    "1. Distinguish between scalars, vectors, and matrices, and explain how each maps to data science concepts (individual measurements, samples, and datasets).\n",
    "2. Compute the dot product of two vectors by hand and explain its role as the core operation behind linear model predictions.\n",
    "3. Apply the matrix multiplication shape rule to predict whether an operation will succeed and determine the shape of the result.\n",
    "4. Convert data between pandas DataFrames and NumPy arrays, recognizing when and why each representation is appropriate.\n",
    "5. Use vectorized NumPy operations instead of Python loops to perform numerical computations efficiently.\n",
    "6. Connect linear algebra operations (matrix-vector multiplication, transpose) to the vectorized gradient descent process introduced in Week 2.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## **Today's Outline**\n",
    "- Lecture  \n",
    "    1. Review of Last Week\n",
    "    2. Why Linear Algebra\n",
    "    3. Scalars, Vectors, and Matrices\n",
    "    4. Vector Operations: The Foundation\n",
    "    5. The Dot Product\n",
    "    6. Conceptual Introduction to Matrix Operations\n",
    "    7. Why NumPy? Speed Through Vectorization\n",
    "    8. From DataFrames to Arrays and Back\n",
    "    9. Connecting Back to Gradient Descent\n",
    "- Break (10-15 Minutes)\n",
    "- Lab (or Homework)\n",
    "- Review\n",
    "    1. Review & Wrap-Up\n",
    "    2. Coming Up\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cda2572",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Opening Reflection**\n",
    "\n",
    "> *\"The heavens declare the glory of God; the skies proclaim the work of his hands. Day after day they pour forth speech; night after night they reveal knowledge.\"*  \n",
    "> — **Psalm 19:1–2 (NIV)**\n",
    "\n",
    "Just as the heavens reveal patterns that declare God's glory, linear algebra gives us the language to uncover hidden patterns in data. Today we learn the mathematical foundation that powers nearly every machine learning algorithm — the ability to represent, transform, and compute with structured numerical data. As we explore dot products and matrix operations, remember that our capacity to discover order in apparent chaos reflects the *Imago Dei* — we are pattern-seekers made in the image of a God of order and wisdom.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5122595c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **1.1 Review of Last Week**\n",
    "\n",
    "Last week we answered a fundamental question: once we define a model, how does it actually *learn* the right parameters? The answer is gradient descent — an iterative algorithm that adjusts model parameters step by step to minimize a loss function.\n",
    "\n",
    "Here are the key ideas you should have coming into today:\n",
    "\n",
    "- **The gradient** is just the slope of the loss function with respect to a parameter. It tells us two things: which *direction* to move (increase or decrease the parameter) and how *steeply* the loss is changing. We always move in the direction that reduces the loss — that is, opposite the sign of the gradient.\n",
    "\n",
    "- **The update rule** follows a simple pattern: subtract the gradient (scaled by a learning rate) from the current parameter value. If the gradient is positive, the parameter decreases. If the gradient is negative, the parameter increases. Either way, the model's predictions get a little closer to the actual values.\n",
    "\n",
    "- **The learning rate (α)** controls step size and requires a balancing act. Too large and the model overshoots the minimum, causing the loss to *increase* or oscillate wildly. Too small and the model inches toward the answer so slowly that training becomes impractical. Finding a good learning rate is one of the first practical skills in machine learning.\n",
    "\n",
    "- **The loss function** (we used Mean Squared Error) gives us a single number that summarizes how wrong the model is across all training examples. Gradient descent's entire job is to make that number smaller, iteration by iteration.\n",
    "\n",
    "- Last week we did all of this with a single feature and a single weight. Today we introduce the linear algebra that lets us scale this process to *many* features and *many* weights simultaneously — which is how real machine learning actually works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260805cd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **1.2 Why Linear Algebra?**\n",
    "\n",
    "Last week we learned how models learn through gradient descent. We saw how a model adjusts its parameters by following the slope of a loss function. But we worked with just **one feature** — a single input variable.\n",
    "\n",
    "Real-world data has **many features**: age, income, square footage, temperature, dozens or hundreds of columns. To work with all of these features simultaneously, we need **linear algebra**.\n",
    "\n",
    "### **Where linear algebra shows up in machine learning:**\n",
    "\n",
    "- **Data representation**: Every dataset is a matrix (rows = samples, columns = features)\n",
    "- **Model predictions**: Computing predictions for all samples at once uses matrix multiplication\n",
    "- **Gradient descent**: Updating multiple weights simultaneously requires vector operations\n",
    "- **Dimensionality reduction** (PCA — Week 12): Built entirely on matrix decomposition\n",
    "- **Neural networks** (Week 13): Every layer is a matrix multiplication + activation\n",
    "\n",
    "**Bottom line:** If you understand vectors, dot products, and the intuition behind matrix operations, you have the mathematical vocabulary for the rest of this course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00834f9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **1.3 Scalars, Vectors, and Matrices**\n",
    "\n",
    "Let's start with the building blocks.\n",
    "\n",
    "| Term | What It Is | Example | NumPy Shape |\n",
    "|------|-----------|---------|-------------|\n",
    "| **Scalar** | A single number | `5`, `3.14`, `-2` | `()` |\n",
    "| **Vector** | An ordered list of numbers | `[1, 2, 3]` | `(3,)` |\n",
    "| **Matrix** | A 2D grid of numbers (rows × columns) | A spreadsheet of data | `(m, n)` |\n",
    "\n",
    "### **How this maps to data science:**\n",
    "- A single measurement (e.g., one person's age) → **scalar**\n",
    "- One row of a dataset (one sample with all its features) → **vector**\n",
    "- An entire dataset (all samples, all features) → **matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bc77c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- Scalar ---\n",
    "temperature = 72.5\n",
    "print(f\"Scalar: {temperature}\")\n",
    "print(f\"  Type: {type(temperature)}\")\n",
    "print()\n",
    "\n",
    "# --- Vector ---\n",
    "# A single house: [sqft, bedrooms, age_years]\n",
    "house_features = np.array([1500, 3, 15])\n",
    "print(f\"Vector: {house_features}\")\n",
    "print(f\"  Shape: {house_features.shape}\")\n",
    "print(f\"  Dimensions: {house_features.ndim}\")\n",
    "print()\n",
    "\n",
    "# --- Matrix ---\n",
    "# 4 houses, each with 3 features: [sqft, bedrooms, age_years]\n",
    "houses = np.array([\n",
    "    [1500, 3, 15],\n",
    "    [2100, 4, 5],\n",
    "    [900, 2, 30],\n",
    "    [1800, 3, 10]\n",
    "])\n",
    "print(f\"Matrix:\\n{houses}\")\n",
    "print(f\"  Shape: {houses.shape}  →  {houses.shape[0]} samples × {houses.shape[1]} features\")\n",
    "print(f\"  Dimensions: {houses.ndim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82db8a0",
   "metadata": {},
   "source": [
    "### **Key Insight: Shape Matters!**\n",
    "\n",
    "In NumPy (and in machine learning), the **shape** of your data tells you everything:\n",
    "\n",
    "- `(n,)` — a 1D vector with `n` elements\n",
    "- `(m, n)` — a matrix with `m` rows and `n` columns\n",
    "- `(m, 1)` — a **column vector** (matrix with 1 column)\n",
    "- `(1, n)` — a **row vector** (matrix with 1 row)\n",
    "\n",
    "Many bugs in data science code come from **shape mismatches**. Always check `.shape`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83f5c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape matters: 1D vector vs. column vector vs. row vector\n",
    "v = np.array([1, 2, 3])\n",
    "col = np.array([[1], [2], [3]])\n",
    "row = np.array([[1, 2, 3]])\n",
    "\n",
    "print(f\"1D vector:     shape = {v.shape}\")\n",
    "print(f\"Column vector: shape = {col.shape}\")\n",
    "print(f\"Row vector:    shape = {row.shape}\")\n",
    "print()\n",
    "print(\"They contain the same numbers, but behave differently in operations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7631d9",
   "metadata": {},
   "source": [
    "---\n",
    "## **1.4 Vector Operations: The Foundation**\n",
    "\n",
    "Before we get to the star of today's lecture (the dot product), let's make sure we understand basic vector operations.\n",
    "\n",
    "### **Element-wise Operations**\n",
    "\n",
    "When you add, subtract, or multiply vectors of the same size, NumPy does it **element by element**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0f4094",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1, 2, 3])\n",
    "b = np.array([4, 5, 6])\n",
    "\n",
    "print(\"Vector a:\", a)\n",
    "print(\"Vector b:\", b)\n",
    "print()\n",
    "\n",
    "# Element-wise operations\n",
    "print(\"Addition (a + b):      \", a + b)       # [1+4, 2+5, 3+6]\n",
    "print(\"Subtraction (a - b):   \", a - b)       # [1-4, 2-5, 3-6]\n",
    "print(\"Multiplication (a * b):\", a * b)        # [1*4, 2*5, 3*6]\n",
    "print(\"Division (a / b):      \", a / b)        # [1/4, 2/5, 3/6]\n",
    "print()\n",
    "\n",
    "# Scalar operations (broadcast the scalar to every element)\n",
    "print(\"Scalar multiply (a * 3):\", a * 3)       # [1*3, 2*3, 3*3]\n",
    "print(\"Scalar add (a + 10):   \", a + 10)       # [1+10, 2+10, 3+10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d4e47a",
   "metadata": {},
   "source": [
    "### **Why element-wise operations matter in data science**\n",
    "\n",
    "When you **standardize** a feature (subtract the mean, divide by standard deviation), you're doing element-wise vector operations on an entire column at once. This is much faster than looping through each value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcf97fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Standardizing a feature (z-score normalization)\n",
    "prices = np.array([150000, 210000, 90000, 180000, 320000])\n",
    "\n",
    "mean_price = prices.mean()\n",
    "std_price = prices.std()\n",
    "\n",
    "# This is an element-wise operation on the entire vector!\n",
    "standardized = (prices - mean_price) / std_price\n",
    "\n",
    "print(f\"Original prices:      {prices}\")\n",
    "print(f\"Mean: ${mean_price:,.0f}   Std: ${std_price:,.0f}\")\n",
    "print(f\"Standardized prices:  {np.round(standardized, 3)}\")\n",
    "print(f\"\\nStandardized mean: {standardized.mean():.6f}  (≈ 0)\")\n",
    "print(f\"Standardized std:  {standardized.std():.6f}  (≈ 1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f0bde9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **1.5 The Dot Product**\n",
    "\n",
    "The **dot product** is the single most important operation in machine learning. It takes two vectors of the same length and produces a **single number** (a scalar).\n",
    "\n",
    "### **Formula**\n",
    "\n",
    "Given two vectors **a** = [a₁, a₂, ..., aₙ] and **b** = [b₁, b₂, ..., bₙ]:\n",
    "\n",
    "$$\\mathbf{a} \\cdot \\mathbf{b} = a_1 b_1 + a_2 b_2 + \\cdots + a_n b_n = \\sum_{i=1}^{n} a_i b_i$$\n",
    "\n",
    "**In words:** Multiply corresponding elements, then sum everything up.\n",
    "\n",
    "### **Three ways to think about the dot product:**\n",
    "\n",
    "1. **Algebraic:** Multiply pairs, add them up (the formula above)\n",
    "2. **Geometric:** Measures how much two vectors point in the same direction\n",
    "3. **Machine Learning:** It's how a linear model makes a prediction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bb7aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dot product: three equivalent ways in NumPy\n",
    "a = np.array([1, 2, 3])\n",
    "b = np.array([4, 5, 6])\n",
    "\n",
    "# Method 1: Manual calculation\n",
    "manual = a[0]*b[0] + a[1]*b[1] + a[2]*b[2]\n",
    "print(f\"Manual:      1×4 + 2×5 + 3×6 = {manual}\")\n",
    "\n",
    "# Method 2: Element-wise multiply then sum\n",
    "multiply_then_sum = np.sum(a * b)\n",
    "print(f\"Sum of a*b:  {multiply_then_sum}\")\n",
    "\n",
    "# Method 3: np.dot() — the standard way\n",
    "dot_result = np.dot(a, b)\n",
    "print(f\"np.dot(a,b): {dot_result}\")\n",
    "\n",
    "# Method 4: The @ operator (Python 3.5+) — most common in modern code\n",
    "at_result = a @ b\n",
    "print(f\"a @ b:       {at_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cfeefc",
   "metadata": {},
   "source": [
    "### **The Dot Product IS a Linear Model Prediction**\n",
    "\n",
    "Remember from Week 1 how a linear model works? Here's the key insight:\n",
    "\n",
    "$$\\hat{y} = w_1 x_1 + w_2 x_2 + w_3 x_3 + b$$\n",
    "\n",
    "This is just:\n",
    "\n",
    "$$\\hat{y} = \\mathbf{w} \\cdot \\mathbf{x} + b$$\n",
    "\n",
    "The dot product of the **weights** vector and the **features** vector, plus a bias term!\n",
    "\n",
    "Let's see this concretely with our housing example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d500e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple house price model\n",
    "# Features: [sqft, bedrooms, age_years]\n",
    "# Weights learned by the model:\n",
    "weights = np.array([150, 20000, -500])  # $ per sqft, $ per bedroom, $ per year of age\n",
    "bias = 50000  # base price\n",
    "\n",
    "# A house to predict: 1500 sqft, 3 bedrooms, 15 years old\n",
    "house = np.array([1500, 3, 15])\n",
    "\n",
    "# Prediction using a loop (slow, verbose)\n",
    "prediction_loop = 0\n",
    "for i in range(len(weights)):\n",
    "    prediction_loop += weights[i] * house[i]\n",
    "prediction_loop += bias\n",
    "\n",
    "# Prediction using the dot product (fast, clean)\n",
    "prediction_dot = weights @ house + bias\n",
    "\n",
    "print(\"House features: [sqft=1500, beds=3, age=15]\")\n",
    "print(f\"\\nManual breakdown:\")\n",
    "print(f\"  150 × 1500  = ${150*1500:>10,}  (sqft contribution)\")\n",
    "print(f\"  20000 × 3   = ${20000*3:>10,}  (bedroom contribution)\")\n",
    "print(f\"  -500 × 15   = ${-500*15:>10,}  (age penalty)\")\n",
    "print(f\"  bias        = ${bias:>10,}\")\n",
    "print(f\"  ─────────────────────────\")\n",
    "print(f\"  Total       = ${prediction_dot:>10,}\")\n",
    "print(f\"\\nUsing dot product: weights @ house + bias = ${prediction_dot:,}\")\n",
    "print(f\"Both methods agree: {prediction_loop == prediction_dot}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d22b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now predict ALL houses at once using matrix-vector multiplication!\n",
    "houses = np.array([\n",
    "    [1500, 3, 15],\n",
    "    [2100, 4, 5],\n",
    "    [900, 2, 30],\n",
    "    [1800, 3, 10]\n",
    "])\n",
    "\n",
    "# Matrix @ vector = vector of predictions\n",
    "all_predictions = houses @ weights + bias\n",
    "\n",
    "print(\"Houses matrix (4 houses × 3 features):\")\n",
    "print(houses)\n",
    "print(f\"\\nWeights: {weights}\")\n",
    "print(f\"Bias:    {bias}\")\n",
    "print(f\"\\nAll predictions at once: {all_predictions}\")\n",
    "print()\n",
    "for i, pred in enumerate(all_predictions):\n",
    "    print(f\"  House {i+1}: ${pred:>10,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4fee85",
   "metadata": {},
   "source": [
    "### **What just happened?**\n",
    "\n",
    "We multiplied a **(4 × 3) matrix** by a **(3,) vector** and got a **(4,) vector** of predictions.\n",
    "\n",
    "Each row of the matrix was dot-producted with the weights vector:\n",
    "\n",
    "```\n",
    "houses (4×3)  @  weights (3,)  =  predictions (4,)\n",
    "\n",
    "[1500, 3, 15]  ·  [150, 20000, -500]  =  327,500\n",
    "[2100, 4,  5]  ·  [150, 20000, -500]  =  442,000\n",
    "[ 900, 2, 30]  ·  [150, 20000, -500]  =  220,000\n",
    "[1800, 3, 10]  ·  [150, 20000, -500]  =  375,000\n",
    "```\n",
    "\n",
    "This is exactly what scikit-learn does under the hood when you call `.predict()` on a `LinearRegression` model!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cab0b2a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **1.6 Conceptual Introduction to Matrix Operations**\n",
    "\n",
    "Now that we understand the dot product, let's zoom out to see the bigger picture of matrix operations.\n",
    "\n",
    "### **Matrix Multiplication (Conceptual)**\n",
    "\n",
    "Matrix multiplication is just **many dot products organized into a grid**.\n",
    "\n",
    "If **A** is (m × n) and **B** is (n × p), then **A @ B** is (m × p).\n",
    "\n",
    "**The inner dimensions must match!** The `n` in (m × **n**) must equal the `n` in (**n** × p).\n",
    "\n",
    "Each element of the result is the dot product of a **row from A** with a **column from B**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167a4236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix multiplication: (m × n) @ (n × p) = (m × p)\n",
    "A = np.array([\n",
    "    [1, 2],\n",
    "    [3, 4],\n",
    "    [5, 6]\n",
    "])  # Shape: (3, 2)\n",
    "\n",
    "B = np.array([\n",
    "    [7, 8, 9],\n",
    "    [10, 11, 12]\n",
    "])  # Shape: (2, 3)\n",
    "\n",
    "C = A @ B  # Shape: (3, 3)\n",
    "\n",
    "print(f\"A shape: {A.shape}\")\n",
    "print(f\"B shape: {B.shape}\")\n",
    "print(f\"C = A @ B shape: {C.shape}\")\n",
    "print(f\"\\nResult:\\n{C}\")\n",
    "print()\n",
    "\n",
    "# Let's verify one element: C[0,0] = row 0 of A · column 0 of B\n",
    "row0_A = A[0, :]       # [1, 2]\n",
    "col0_B = B[:, 0]       # [7, 10]\n",
    "print(f\"Verifying C[0,0]:\")\n",
    "print(f\"  Row 0 of A: {row0_A}\")\n",
    "print(f\"  Col 0 of B: {col0_B}\")\n",
    "print(f\"  Dot product: {row0_A[0]}×{col0_B[0]} + {row0_A[1]}×{col0_B[1]} = {row0_A @ col0_B}\")\n",
    "print(f\"  C[0,0] = {C[0,0]}  ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674ffe4b",
   "metadata": {},
   "source": [
    "### **The Shape Rule (Memorize This!)**\n",
    "\n",
    "```\n",
    "(m × n) @ (n × p) = (m × p)  \n",
    "     ↑     ↑  \n",
    "These must match!\n",
    "```\n",
    "\n",
    "If the inner dimensions don't match, you get an error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3556e9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What happens when shapes don't match?\n",
    "X = np.array([[1, 2, 3]])       # (1, 3)\n",
    "Y = np.array([[4, 5, 6]])       # (1, 3)\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"Y shape: {Y.shape}\")\n",
    "print()\n",
    "\n",
    "try:\n",
    "    result = X @ Y\n",
    "except ValueError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"   (1×3) @ (1×3) fails because inner dimensions 3 ≠ 1\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Fix: transpose Y so shapes align\n",
    "result = X @ Y.T  # (1×3) @ (3×1) = (1×1)\n",
    "print(f\"X @ Y.T = {result}  ← this is just the dot product of X and Y!\")\n",
    "print(f\"  Shape: {result.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe077d6",
   "metadata": {},
   "source": [
    "### **Transpose**\n",
    "\n",
    "The **transpose** of a matrix flips it over its diagonal — rows become columns and columns become rows.\n",
    "\n",
    "- Shape (m × n) → (n × m)\n",
    "- In NumPy: `A.T` or `np.transpose(A)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0461faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6]\n",
    "])\n",
    "\n",
    "print(f\"A (shape {A.shape}):\")\n",
    "print(A)\n",
    "print(f\"\\nA.T (shape {A.T.shape}):\")\n",
    "print(A.T)\n",
    "print()\n",
    "print(\"Notice: Row 0 of A [1,2,3] became Column 0 of A.T\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed949d0",
   "metadata": {},
   "source": [
    "### **Where transpose shows up in ML:**\n",
    "\n",
    "- **Normal equation** for linear regression: $\\mathbf{w} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}$\n",
    "- **Covariance matrix**: $\\frac{1}{n} \\mathbf{X}^T \\mathbf{X}$ (used in PCA — Week 12)\n",
    "- **Batch predictions**: Sometimes you need to transpose your data to make shapes align\n",
    "\n",
    "You don't need to memorize these formulas right now — just recognize that the transpose is everywhere in ML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a344fca",
   "metadata": {},
   "source": [
    "---\n",
    "## **1.7 Why NumPy? Speed Through Vectorization**\n",
    "\n",
    "A natural question: \"Why not just use Python lists and for-loops?\"\n",
    "\n",
    "**Answer: Speed.** NumPy operations are **vectorized** — they run in optimized C code instead of slow Python loops. The difference is dramatic, especially with large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dc7008",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Create a large dataset: 100,000 samples, 50 features\n",
    "np.random.seed(42)\n",
    "n_samples = 100_000\n",
    "n_features = 50\n",
    "\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "w = np.random.randn(n_features)\n",
    "\n",
    "print(f\"Data: {n_samples:,} samples × {n_features} features\")\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"w shape: {w.shape}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be77881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Pure Python loops\n",
    "start = time.time()\n",
    "predictions_loop = []\n",
    "for i in range(n_samples):\n",
    "    pred = 0\n",
    "    for j in range(n_features):\n",
    "        pred += X[i, j] * w[j]\n",
    "    predictions_loop.append(pred)\n",
    "loop_time = time.time() - start\n",
    "\n",
    "print(f\"Python loops: {loop_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf57798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: NumPy dot product (vectorized)\n",
    "start = time.time()\n",
    "predictions_numpy = X @ w\n",
    "numpy_time = time.time() - start\n",
    "\n",
    "print(f\"NumPy (X @ w): {numpy_time:.6f} seconds\")\n",
    "print(f\"\\nSpeedup: {loop_time / numpy_time:.0f}x faster!\")\n",
    "print(f\"\\nResults match: {np.allclose(predictions_loop, predictions_numpy)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f419a0f",
   "metadata": {},
   "source": [
    "### **Why is NumPy so fast?**\n",
    "\n",
    "1. **Compiled C code**: NumPy's core is written in C, not Python\n",
    "2. **Contiguous memory**: NumPy arrays are stored in a single block of memory (unlike Python lists)\n",
    "3. **SIMD instructions**: Modern CPUs can process multiple numbers in a single instruction\n",
    "4. **No type checking**: Python checks the type of every variable at every step; NumPy doesn't\n",
    "\n",
    "**Rule of thumb:** If you find yourself writing a `for` loop over array elements in data science code, there's almost certainly a vectorized NumPy operation that does the same thing faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9df8d49",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **1.8 From DataFrames to Arrays and Back**\n",
    "\n",
    "In practice, you'll receive data as a pandas DataFrame (from CSV, database, etc.) but scikit-learn models need NumPy arrays under the hood. Let's see how to move between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bab1604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'sqft': [1500, 2100, 900, 1800, 2500],\n",
    "    'bedrooms': [3, 4, 2, 3, 5],\n",
    "    'age': [15, 5, 30, 10, 2],\n",
    "    'price': [327500, 442000, 220000, 375000, 510000]\n",
    "})\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "print(f\"\\nType: {type(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04a90dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame → NumPy array\n",
    "X_df = df[['sqft', 'bedrooms', 'age']]     # Feature columns (still a DataFrame)\n",
    "y_df = df['price']                         # Target column (still a Series)\n",
    "\n",
    "# Convert to NumPy\n",
    "X_array = X_df.to_numpy()  # or X_df.values (older style)\n",
    "y_array = y_df.to_numpy()\n",
    "\n",
    "print(f\"X as DataFrame:\\n{X_df}\")\n",
    "print(f\"\\nX as NumPy array:\\n{X_array}\")\n",
    "print(f\"\\nShape: {X_array.shape}\")\n",
    "print(f\"dtype: {X_array.dtype}\")\n",
    "\n",
    "print(f\"y as DataFrame:\\n{y_df}\")\n",
    "print(f\"\\ny as NumPy array:\\n{y_array}\")\n",
    "print(f\"\\nShape: {y_array.shape}\")\n",
    "print(f\"dtype: {y_array.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c949d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What scikit-learn does internally:\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_df, y_df)  # Scikit-learn accepts DataFrames and converts internally\n",
    "\n",
    "print(f\"Model weights (coef_): {model.coef_}\")\n",
    "print(f\"Model bias (intercept_): {model.intercept_:.2f}\")\n",
    "print(f\"\\nThese are NumPy arrays:\")\n",
    "print(f\"  coef_ type:      {type(model.coef_)}\")\n",
    "print(f\"  intercept_ type: {type(model.intercept_)}\")\n",
    "print()\n",
    "\n",
    "# The prediction is literally: X @ coef_ + intercept_\n",
    "manual_preds = X_array @ model.coef_ + model.intercept_\n",
    "sklearn_preds = model.predict(X_df)\n",
    "\n",
    "print(\"Manual (X @ w + b) vs sklearn .predict():\")\n",
    "print(f\"  Match: {np.allclose(manual_preds, sklearn_preds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188b29a1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **1.9 Connecting Back to Gradient Descent (Week 2)**\n",
    "\n",
    "Last week we updated a single weight using gradient descent. With linear algebra, we can update **all weights at once**.\n",
    "\n",
    "### **Single feature (Week 2):**\n",
    "```python\n",
    "prediction = w * x + b\n",
    "error = prediction - y\n",
    "w = w - learning_rate * error * x\n",
    "```\n",
    "\n",
    "### **Multiple features (with linear algebra):**\n",
    "```python\n",
    "predictions = X @ w + b           # dot product: all predictions at once\n",
    "errors = predictions - y          # vector of errors\n",
    "gradient = X.T @ errors / n       # all gradients at once!\n",
    "w = w - learning_rate * gradient  # update all weights simultaneously\n",
    "```\n",
    "\n",
    "The `X.T @ errors` line is computing the gradient for every weight simultaneously using matrix multiplication. This is **vectorized gradient descent**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b04bcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorized gradient descent for multiple linear regression\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simple synthetic data: y = 3*x1 + 5*x2 + 7 + noise\n",
    "n = 200\n",
    "X_train = np.random.randn(n, 2)  # 200 samples, 2 features\n",
    "true_weights = np.array([3.0, 5.0])\n",
    "true_bias = 7.0\n",
    "y_train = X_train @ true_weights + true_bias + np.random.randn(n) * 0.5\n",
    "\n",
    "# Initialize weights\n",
    "w = np.zeros(2)\n",
    "b = 0.0\n",
    "lr = 0.05\n",
    "\n",
    "# Train with vectorized gradient descent\n",
    "print(\"Epoch | w1     | w2     | bias   | MSE\")\n",
    "print(\"------+--------+--------+--------+--------\")\n",
    "for epoch in range(n+1):\n",
    "    # Forward pass (dot product!)\n",
    "    predictions = X_train @ w + b\n",
    "\n",
    "    # Compute error\n",
    "    errors = predictions - y_train\n",
    "    mse = np.mean(errors ** 2)\n",
    "\n",
    "    # Compute gradients (matrix transpose!)\n",
    "    grad_w = (X_train.T @ errors) / n     # gradient for all weights at once\n",
    "    grad_b = np.mean(errors)              # gradient for bias\n",
    "\n",
    "    # Update\n",
    "    w = w - lr * grad_w\n",
    "    b = b - lr * grad_b\n",
    "\n",
    "    if epoch % 40 == 0:\n",
    "        print(f\"{epoch:>5} | {w[0]:>6.3f} | {w[1]:>6.3f} | {b:>6.3f} | {mse:.4f}\")\n",
    "\n",
    "print(f\"\\nLearned:  w = {w},  b = {b:.3f}\")\n",
    "print(f\"True:     w = {true_weights}, b = {true_bias}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e701518f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **BREAK (10-15 minutes)**\n",
    "\n",
    "---\n",
    "\n",
    "## **2.1 Lab Exercises** (new notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4b7a1a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **3.1: Review & Wrap-Up**\n",
    "\n",
    "### **3.1.1 What We Learned Today**\n",
    "\n",
    "| Concept | Why It Matters |\n",
    "|---------|----------------|\n",
    "| **Vectors** | A single data sample is a vector of features |\n",
    "| **Matrices** | An entire dataset is a matrix (samples × features) |\n",
    "| **Dot product** | This IS how linear models make predictions: **w · x + b** |\n",
    "| **Matrix × vector** | Predict all samples at once: **X @ w + b** |\n",
    "| **Transpose** | Needed for computing gradients: **X.T @ errors** |\n",
    "| **Vectorization** | NumPy is 100–1000× faster than Python loops |\n",
    "| **DataFrame ↔ Array** | Pandas for loading/exploring; NumPy for computing |\n",
    "\n",
    "### **3.1.2 Discussion Questions**\n",
    "1. In your own words, what does the dot product compute? Why is it useful in ML?\n",
    "2. If your data matrix X has shape `(500, 10)` and your weight vector w has shape `(10,)`, what shape is `X @ w`?\n",
    "3. Why would `X @ w` fail if w had shape `(5,)` instead?\n",
    "4. When would you use `X.T` (transpose) in a machine learning context?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84824fa6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **3.2 Coming Up**\n",
    "\n",
    "### **3.2.1 Next Week Preview: Multiple Linear Regression & Regularization**\n",
    "- We'll use today's linear algebra to build models with many features\n",
    "- We'll learn about **overfitting** and how **L1 and L2 regularization** prevent it\n",
    "- Key question: What happens when you have too many features?\n",
    "\n",
    "### **3.2.2 Homework Reminder**\n",
    "- **Lab notebook due:** Monday, 16 February @ 6:00 PM (grace period until Wed, 18 February @ 11:59 PM)\n",
    "- Next week begins with a **mastery assessment** on this week's material (dot products, shapes, vectorization)\n",
    "- Practice computing dot products by hand — you'll need to do this without AI!\n",
    "\n",
    "### **3.2.3 Looking Ahead**\n",
    "- **Week 4 (Multiple Linear Regression):** We'll use everything from today to build multi-feature regression models\n",
    "- **Week 12 (PCA):** Matrix decomposition for dimensionality reduction\n",
    "- **Week 13 (Neural Networks):** Every layer is a matrix multiplication + activation function\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py314",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
