{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3573d1d",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "\n",
    "# **Spring 2026 &mdash; CIS 3813<br>Advanced Data Science<br>(Introduction to Machine Learning)**\n",
    "### Week 2: How Models Learn\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7803af9",
   "metadata": {},
   "source": [
    "**Date:** 02 February 2026\n",
    "**Time:** 6:00–9:00 PM  \n",
    "**Instructor:** Dr. Patrick T. Marsh  \n",
    "**Course Verse:** \"He has shown you, O mortal, what is good. And what does the Lord require of you? To act justly and to love mercy and to walk humbly with your God.\"  &mdash; *Micah 6:8 (NIV)*\n",
    "\n",
    "---\n",
    "## **Week 2 Learning Objectives**\n",
    "\n",
    "By the end of this lecture, you will be able to:\n",
    "\n",
    "1. **Explain** the concept of a loss function and why it matters for model training\n",
    "2. **Describe** how gradient descent uses derivatives to find optimal parameters\n",
    "3. **Visualize** the gradient descent process in 1D and 2D\n",
    "4. **Implement** a basic gradient descent algorithm from scratch\n",
    "5. **Identify** the role of the learning rate in convergence\n",
    "---\n",
    "\n",
    "\n",
    "## **Today's Outline**\n",
    "- Lecture  \n",
    "    1. Review of Last Week  \n",
    "    2. The Learning Problem  \n",
    "    3. Gradient Descent  \n",
    "    4. \"Animated\" Gradient Descent\n",
    "    5. Connection to Scikit-Learn\n",
    "    6. Reflection Question\n",
    "- Break (10-15 Minutes)\n",
    "- Lab (or Homework)\n",
    "- Review\n",
    "    1. Key Concepts Summary\n",
    "    2. Preview: Next Week\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cda2572",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Opening Reflection**\n",
    "\n",
    "> *\"I press on toward the goal for the prize of the upward call of God in Christ Jesus.\"*  \n",
    "> — **Philippians 3:14 (ESV)**\n",
    "\n",
    "Today we learn how machine learning models \"press on toward the goal\" of finding optimal solutions. Just as Paul describes his spiritual journey as a continuous pursuit—not arriving instantly but pressing forward step by step—gradient descent algorithms take iterative steps toward their objective. Each step brings the model closer to the goal, even when the path isn't immediately clear.\n",
    "\n",
    "In our Christian walk, we don't achieve perfection instantaneously; we grow through persistent effort, guided by wisdom. Similarly, our models learn through persistent iteration, guided by mathematics. This week, we'll see how this process works under the hood.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5122595c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **1.1 Review of Last Week**\n",
    "\n",
    "### **1.1.1 What is a Function?**\n",
    "\n",
    "A **function** maps inputs to outputs. In machine learning:\n",
    "- **Input**: Features (data) + Parameters (weights)\n",
    "- **Output**: Predictions or loss values\n",
    "\n",
    "For example, a simple linear model:\n",
    "\n",
    "$$\\hat{y} = mx + b$$\n",
    "\n",
    "Where:\n",
    "- $x$ is our input feature\n",
    "- $m$ is the slope (weight)\n",
    "- $b$ is the intercept (bias)\n",
    "- $\\hat{y}$ is our prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6956d0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with our imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345369c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: A simple linear function\n",
    "def linear_function(x, m=2, b=1):\n",
    "    \"\"\"A simple linear function: y = mx + b\"\"\"\n",
    "    return m * x + b\n",
    "\n",
    "# Let's visualize it\n",
    "x = np.linspace(-5, 5, 100)\n",
    "y = linear_function(x)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, y, 'b-', linewidth=2, label='$y = 2x + 1$')\n",
    "plt.axhline(y=0, color='k', linewidth=0.5)\n",
    "plt.axvline(x=0, color='k', linewidth=0.5)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('A Simple Linear Function')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb9904a",
   "metadata": {},
   "source": [
    "### **1.1.2 What is Slope? (The Derivative)**\n",
    "\n",
    "The **slope** tells us how steep a function is at any point, and **in which direction** it's increasing.\n",
    "\n",
    "For a linear function $y = mx + b$:\n",
    "- The slope is constant: $\\frac{dy}{dx} = m$\n",
    "\n",
    "For non-linear functions, the slope changes at every point. The **derivative** gives us the instantaneous rate of change.\n",
    "\n",
    "#### **Key Insight for Machine Learning:**\n",
    "- **Positive slope**: Function is increasing → move left to decrease\n",
    "- **Negative slope**: Function is decreasing → move right to decrease\n",
    "- **Zero slope**: We're at a critical point! (Potentially a minimum or maximum or a saddle point <-- Thanks Travis!)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3dffbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize slope with a quadratic function\n",
    "def quadratic(x):\n",
    "    \"\"\"A simple quadratic: f(x) = x^2\"\"\"\n",
    "    return x ** 2\n",
    "\n",
    "def quadratic_derivative(x):\n",
    "    \"\"\"Derivative of x^2 is 2x\"\"\"\n",
    "    return 2 * x\n",
    "\n",
    "x = np.linspace(-3, 3, 100)\n",
    "y = quadratic(x)\n",
    "\n",
    "# Plot the function\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left plot: the function with tangent lines\n",
    "axes[0].plot(x, y, 'b-', linewidth=2, label='$f(x) = x^2$')\n",
    "\n",
    "# Add tangent lines at a few points\n",
    "points = [-2, 0, 2]\n",
    "colors = ['red', 'green', 'purple']\n",
    "for pt, color in zip(points, colors):\n",
    "    slope = quadratic_derivative(pt)\n",
    "    y_pt = quadratic(pt)\n",
    "    # Tangent line: y - y_pt = slope * (x - pt)\n",
    "    tangent_x = np.linspace(pt - 1, pt + 1, 50)\n",
    "    tangent_y = slope * (tangent_x - pt) + y_pt\n",
    "    axes[0].plot(tangent_x, tangent_y, color=color, linestyle='--',\n",
    "                 label=f'Tangent at x={pt}, slope={slope}')\n",
    "    axes[0].scatter([pt], [y_pt], color=color, s=100, zorder=5)\n",
    "\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('f(x)')\n",
    "axes[0].set_title('Quadratic Function with Tangent Lines')\n",
    "axes[0].legend()\n",
    "axes[0].set_ylim(-2, 10)\n",
    "\n",
    "# Right plot: the derivative\n",
    "axes[1].plot(x, quadratic_derivative(x), 'r-', linewidth=2, label=\"$f'(x) = 2x$\")\n",
    "axes[1].axhline(y=0, color='k', linewidth=0.5)\n",
    "axes[1].axvline(x=0, color='k', linewidth=0.5)\n",
    "for pt, color in zip(points, colors):\n",
    "    axes[1].scatter([pt], [quadratic_derivative(pt)], color=color, s=100, zorder=5)\n",
    "axes[1].set_xlabel('x')\n",
    "axes[1].set_ylabel(\"f'(x)\")\n",
    "axes[1].set_title('The Derivative (Slope at Each Point)')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f07d48",
   "metadata": {},
   "source": [
    "### Discussion Question\n",
    "\n",
    "Look at the right plot above. At what x-value is the slope zero? What does this correspond to on the left plot?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512625c8",
   "metadata": {},
   "source": [
    "---\n",
    "## **1.2 The Learning Problem**\n",
    "\n",
    "### **1.2.1 What Does \"Learning\" Mean?**\n",
    "\n",
    "In machine learning, \"learning\" means finding the **best parameters** for our model.\n",
    "\n",
    "**Example**: For a linear model $\\hat{y} = mx + b$:\n",
    "- We have data: $(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)$\n",
    "- We want to find the values of $m$ and $b$ that make our predictions $\\hat{y}$ as close to the actual $y$ values as possible\n",
    "\n",
    "But how do we measure \"close\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86714b7e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5fc574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create some sample data\n",
    "np.random.seed(42)  # For reproducibility\n",
    "\n",
    "# True relationship: y = 3x + 2 + noise\n",
    "true_m = 3\n",
    "true_b = 2\n",
    "n_samples = 50\n",
    "\n",
    "X = np.random.uniform(-5, 5, n_samples)\n",
    "y = true_m * X + true_b + np.random.normal(0, 2, n_samples)\n",
    "\n",
    "# Visualize the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X, y, alpha=0.7, label='Data points')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Our Dataset (True relationship: y = 3x + 2 + noise)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aacb10c",
   "metadata": {},
   "source": [
    "### **1.2.2 The Loss Function (Cost Function)**\n",
    "\n",
    "A **loss function** measures how wrong our predictions are. The most common for regression is **Mean Squared Error (MSE)**:\n",
    "\n",
    "$$\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - (mx_i + b))^2$$\n",
    "\n",
    "#### **Why squared error?**\n",
    "1. **Penalizes large errors more** than small errors\n",
    "2. **Differentiable** everywhere (smooth curve)\n",
    "3. **Always positive** (no cancellation of positive and negative errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb03adcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse(m, b, X, y):\n",
    "    \"\"\"Compute Mean Squared Error for given parameters.\"\"\"\n",
    "    predictions = m * X + b\n",
    "    errors = y - predictions\n",
    "    mse = np.mean(errors ** 2)\n",
    "    return mse\n",
    "\n",
    "# Let's see how MSE changes with different parameter choices\n",
    "params = [(true_m, true_b, 'True Parameters'), (1, 0, 'Poor Fit #1'),\n",
    "          (5, 5, 'Poor Fit #2'), (0, 0, 'Terrible Fit')]\n",
    "\n",
    "print(f\"True parameters: m={params[0][0]}, b={params[0][1]}\")\n",
    "print(f\"MSE with true parameters: {compute_mse(params[0][0], params[0][1], X, y):.4f}\")\n",
    "print(f\"MSE with m=1, b=0: {compute_mse(params[1][0], params[1][1], X, y):.4f}\")\n",
    "print(f\"MSE with m=5, b=5: {compute_mse(params[2][0], params[2][1], X, y):.4f}\")\n",
    "print(f\"MSE with m=0, b=0: {compute_mse(params[3][0], params[3][1], X, y):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4a67d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize different model fits\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "x_line = np.linspace(-5, 5, 100)\n",
    "\n",
    "for ax, (m, b, title) in zip(axes.flat, params):\n",
    "    ax.scatter(X, y, alpha=0.5, label='Data')\n",
    "    ax.plot(x_line, m * x_line + b, 'r-', linewidth=2, label=f'y = {m}x + {b}')\n",
    "    mse = compute_mse(m, b, X, y)\n",
    "    ax.set_title(f'{title}\\nMSE = {mse:.2f}')\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.legend()\n",
    "    ax.set_xlim(-6, 6)\n",
    "    ax.set_ylim(-20, 25)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58468ed",
   "metadata": {},
   "source": [
    "### **1.2.3 The Loss Landscape**\n",
    "\n",
    "For a model with parameters $m$ and $b$, the loss function creates a **surface** over the parameter space.\n",
    "\n",
    "Our goal: Find the lowest point on this surface!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a886e4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a grid of m and b values\n",
    "m_range = np.linspace(-2, 8, 100)\n",
    "b_range = np.linspace(-5, 10, 100)\n",
    "M, B = np.meshgrid(m_range, b_range)\n",
    "\n",
    "# Compute MSE for each combination\n",
    "Z = np.zeros_like(M)\n",
    "for i in range(M.shape[0]):\n",
    "    for j in range(M.shape[1]):\n",
    "        Z[i, j] = compute_mse(M[i, j], B[i, j], X, y)\n",
    "\n",
    "# Create 3D visualization\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "\n",
    "# 3D Surface\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "surf = ax1.plot_surface(M, B, Z, cmap=cm.viridis, alpha=0.8)\n",
    "ax1.scatter([true_m], [true_b], [compute_mse(true_m, true_b, X, y)],\n",
    "            color='red', s=200, label='True minimum')\n",
    "ax1.set_xlabel('m (slope)')\n",
    "ax1.set_ylabel('b (intercept)')\n",
    "ax1.set_zlabel('MSE', rotation=90)\n",
    "ax1.set_title('Loss Landscape (3D View)')\n",
    "# ax1.view_init(elev=0, azim=-30, roll=0)\n",
    "\n",
    "# Contour plot\n",
    "ax2 = fig.add_subplot(122)\n",
    "contour = ax2.contour(M, B, Z, levels=30, cmap=cm.viridis)\n",
    "ax2.scatter([true_m], [true_b], color='red', s=200, marker='*',\n",
    "            label='True minimum', zorder=5)\n",
    "ax2.set_xlabel('m (slope)')\n",
    "ax2.set_ylabel('b (intercept)')\n",
    "ax2.set_title('Loss Landscape (Contour View)')\n",
    "ax2.legend()\n",
    "plt.colorbar(contour, ax=ax2, label='MSE')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c3cf77",
   "metadata": {},
   "source": [
    "#### **Key Observation**\n",
    "\n",
    "The loss surface looks like a **bowl**! There's a clear minimum at the bottom. Our goal is to find this minimum.\n",
    "\n",
    "But how do we find it without trying every possible combination of parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b8316d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **1.3 Gradient Descent**\n",
    "\n",
    "### **1.3.1 The Core Idea**\n",
    "\n",
    "Imagine you're blindfolded on a hilly landscape, trying to find the lowest point.\n",
    "\n",
    "**Strategy**: Feel the slope beneath your feet. Take a step in the downhill direction. Repeat.\n",
    "\n",
    "This is exactly what **gradient descent** does!\n",
    "\n",
    "#### **The Algorithm**\n",
    "\n",
    "1. Start with random parameter values\n",
    "2. Compute the gradient (slope) of the loss function\n",
    "3. Take a step in the **opposite** direction of the gradient (downhill)\n",
    "4. Repeat until convergence\n",
    "\n",
    "#### **The Math**\n",
    "\n",
    "$$\\theta_{\\text{new}} = \\theta_{\\text{old}} - \\alpha \\cdot \\nabla L(\\theta)$$\n",
    "\n",
    "Where:\n",
    "- $\\theta$ = parameters (m, b in our case)\n",
    "- $\\alpha$ = learning rate (step size)\n",
    "- $\\nabla L(\\theta)$ = gradient of the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec6b069",
   "metadata": {},
   "source": [
    "### **1.3.2 Let's Start Simple: 1D Gradient Descent**\n",
    "\n",
    "First, let's understand gradient descent with a simple function: $f(x) = x^2$\n",
    "\n",
    "We want to find the value of $x$ that minimizes this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c59eec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    \"\"\"Our simple function to minimize: f(x) = x^2\"\"\"\n",
    "    return x ** 2\n",
    "\n",
    "def df(x):\n",
    "    \"\"\"Derivative: f'(x) = 2x\"\"\"\n",
    "    return 2 * x\n",
    "\n",
    "def gradient_descent_1d(start, learning_rate, n_iterations):\n",
    "    \"\"\"Perform gradient descent on f(x) = x^2\"\"\"\n",
    "    x = start\n",
    "    history = [(x, f(x))]  # Track our path\n",
    "\n",
    "    for i in range(n_iterations):\n",
    "        gradient = df(x)           # Compute the gradient\n",
    "        x = x - learning_rate * gradient  # Take a step\n",
    "        history.append((x, f(x)))\n",
    "\n",
    "    return x, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343c67af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run gradient descent\n",
    "start_point = 4.0\n",
    "learning_rate = 0.1\n",
    "n_iterations = 200\n",
    "\n",
    "final_x, history = gradient_descent_1d(start_point, learning_rate, n_iterations)\n",
    "\n",
    "print(f\"Starting point: x = {start_point}\")\n",
    "print(f\"Final point after {n_iterations} iterations: x = {final_x:.6f}\")\n",
    "print(f\"Final f(x): {f(final_x):.10f}\")\n",
    "print(f\"\\nTrue minimum: x = 0, f(x) = 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b26abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the gradient descent process\n",
    "x_plot = np.linspace(-5, 5, 100)\n",
    "y_plot = f(x_plot)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Show the path on the function\n",
    "axes[0].plot(x_plot, y_plot, 'b-', linewidth=2, label='$f(x) = x^2$')\n",
    "x_hist = [h[0] for h in history]\n",
    "y_hist = [h[1] for h in history]\n",
    "axes[0].scatter(x_hist, y_hist, c=range(len(history)), cmap='Reds', s=100, zorder=5)\n",
    "axes[0].plot(x_hist, y_hist, 'r--', alpha=0.5)\n",
    "axes[0].scatter([start_point], [f(start_point)], color='green', s=200,\n",
    "                marker='o', label='Start', zorder=6)\n",
    "axes[0].scatter([final_x], [f(final_x)], color='red', s=200,\n",
    "                marker='*', label='End', zorder=6)\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('f(x)')\n",
    "axes[0].set_title('Gradient Descent on $f(x) = x^2$')\n",
    "axes[0].legend()\n",
    "\n",
    "# Right: Show convergence over iterations\n",
    "axes[1].plot(range(len(history)), y_hist, 'b-o', linewidth=2, markersize=5)\n",
    "axes[1].set_xlabel('Iteration')\n",
    "axes[1].set_ylabel('f(x)')\n",
    "axes[1].set_title('Loss Over Iterations')\n",
    "axes[1].set_yscale('log')  # Log scale to see small values\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acbd312",
   "metadata": {},
   "source": [
    "## **1.3.3 The Importance of Learning Rate**\n",
    "\n",
    "The **learning rate** ($\\alpha$) controls how big our steps are.\n",
    "\n",
    "- **Too small**: Takes forever to converge\n",
    "- **Too large**: May overshoot and never converge\n",
    "- **Just right**: Fast convergence to the minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd34cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different learning rates\n",
    "learning_rates = [0.01, 0.1, 0.5, 0.95]\n",
    "start = 4.0\n",
    "n_iterations = 20\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "x_plot = np.linspace(-5, 5, 100)\n",
    "\n",
    "for ax, lr in zip(axes.flat, learning_rates):\n",
    "    final_x, history = gradient_descent_1d(start, lr, n_iterations)\n",
    "    x_hist = [h[0] for h in history]\n",
    "    y_hist = [h[1] for h in history]\n",
    "\n",
    "    ax.plot(x_plot, f(x_plot), 'b-', linewidth=2, label='$f(x) = x^2$')\n",
    "    ax.scatter(x_hist, y_hist, c=range(len(history)), cmap='Reds', s=80, zorder=5)\n",
    "    ax.plot(x_hist, y_hist, 'r--', alpha=0.5)\n",
    "    ax.scatter([start], [f(start)], color='green', s=150, marker='o', zorder=6)\n",
    "    ax.scatter([final_x], [f(final_x)], color='red', s=150, marker='*', zorder=6)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('f(x)')\n",
    "    ax.set_title(f'Learning Rate = {lr}\\nFinal x = {final_x:.4f}')\n",
    "    ax.set_xlim(-5, 5)\n",
    "    ax.set_ylim(-1, 20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba6f611",
   "metadata": {},
   "source": [
    "#### **Discussion Questions**\n",
    "\n",
    "1. What happens with a learning rate of 0.01? Is it converging?\n",
    "2. What happens with a learning rate of 0.95? Do you see oscillation?\n",
    "3. Which learning rate seems \"just right\" for this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c33ce8a",
   "metadata": {},
   "source": [
    "### **1.3.4 Gradient Descent for Linear Regression**\n",
    "\n",
    "Now let's apply this to our actual problem: finding the best $m$ and $b$ for linear regression.\n",
    "\n",
    "#### **The Gradients**\n",
    "\n",
    "For MSE loss: $L = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - (mx_i + b))^2$\n",
    "\n",
    "The partial derivatives are:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial m} = \\frac{-2}{n} \\sum_{i=1}^{n} x_i(y_i - (mx_i + b))$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial b} = \\frac{-2}{n} \\sum_{i=1}^{n} (y_i - (mx_i + b))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62210d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(m, b, X, y):\n",
    "    \"\"\"Compute the gradients of MSE with respect to m and b.\"\"\"\n",
    "    n = len(X)\n",
    "    predictions = m * X + b\n",
    "    errors = y - predictions\n",
    "\n",
    "    # Partial derivatives\n",
    "    dm = (-2/n) * np.sum(X * errors)\n",
    "    db = (-2/n) * np.sum(errors)\n",
    "\n",
    "    return dm, db\n",
    "\n",
    "def gradient_descent_linear(X, y, learning_rate=0.01, n_iterations=1000):\n",
    "    \"\"\"Perform gradient descent to find optimal m and b.\"\"\"\n",
    "    # Initialize with random values\n",
    "    m = np.random.randn()\n",
    "    b = np.random.randn()\n",
    "\n",
    "    history = []\n",
    "\n",
    "    for i in range(n_iterations):\n",
    "        # Compute gradients\n",
    "        dm, db = compute_gradients(m, b, X, y)\n",
    "\n",
    "        # Update parameters\n",
    "        m = m - learning_rate * dm\n",
    "        b = b - learning_rate * db\n",
    "\n",
    "        # Track history\n",
    "        mse = compute_mse(m, b, X, y)\n",
    "        history.append({'m': m, 'b': b, 'mse': mse})\n",
    "\n",
    "        # Print progress occasionally\n",
    "        if (i + 1) % 100 == 0 or i == 0:\n",
    "            print(f\"Iteration {i+1:4d}: m = {m:.4f}, b = {b:.4f}, MSE = {mse:.4f}\")\n",
    "\n",
    "    return m, b, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269f0868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run gradient descent on our data\n",
    "np.random.seed(42)\n",
    "final_m, final_b, history = gradient_descent_linear(X, y, learning_rate=0.01, n_iterations=500)\n",
    "\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(f\"Final parameters: m = {final_m:.4f}, b = {final_b:.4f}\")\n",
    "print(f\"True parameters:  m = {true_m}, b = {true_b}\")\n",
    "print(f\"Final MSE: {history[-1]['mse']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed9d4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the learning process\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Left: Final fit\n",
    "axes[0].scatter(X, y, alpha=0.6, label='Data')\n",
    "x_line = np.linspace(-5, 5, 100)\n",
    "axes[0].plot(x_line, final_m * x_line + final_b, 'r-', linewidth=2,\n",
    "             label=f'Learned: y = {final_m:.2f}x + {final_b:.2f}')\n",
    "axes[0].plot(x_line, true_m * x_line + true_b, 'g--', linewidth=2,\n",
    "             label=f'True: y = {true_m}x + {true_b}')\n",
    "axes[0].set_xlabel('X')\n",
    "axes[0].set_ylabel('y')\n",
    "axes[0].set_title('Final Model Fit')\n",
    "axes[0].legend()\n",
    "\n",
    "# Middle: Loss over time\n",
    "mse_history = [h['mse'] for h in history]\n",
    "axes[1].plot(mse_history, 'b-', linewidth=2)\n",
    "axes[1].set_xlabel('Iteration')\n",
    "axes[1].set_ylabel('MSE')\n",
    "axes[1].set_title('Loss Over Training')\n",
    "axes[1].set_yscale('log')\n",
    "\n",
    "# Right: Path through parameter space\n",
    "m_hist = [h['m'] for h in history]\n",
    "b_hist = [h['b'] for h in history]\n",
    "\n",
    "# Create contour background\n",
    "m_range = np.linspace(-1, 6, 100)\n",
    "b_range = np.linspace(-3, 7, 100)\n",
    "M, B = np.meshgrid(m_range, b_range)\n",
    "Z = np.zeros_like(M)\n",
    "for i in range(M.shape[0]):\n",
    "    for j in range(M.shape[1]):\n",
    "        Z[i, j] = compute_mse(M[i, j], B[i, j], X, y)\n",
    "\n",
    "axes[2].contour(M, B, Z, levels=30, cmap='viridis', alpha=0.5)\n",
    "axes[2].plot(m_hist, b_hist, 'r-', linewidth=1, alpha=0.7)\n",
    "axes[2].scatter(m_hist[::50], b_hist[::50], c='red', s=50, zorder=5)\n",
    "axes[2].scatter([m_hist[0]], [b_hist[0]], color='green', s=200,\n",
    "                marker='o', label='Start', zorder=6)\n",
    "axes[2].scatter([m_hist[-1]], [b_hist[-1]], color='red', s=200,\n",
    "                marker='*', label='End', zorder=6)\n",
    "axes[2].scatter([true_m], [true_b], color='blue', s=200,\n",
    "                marker='X', label='True', zorder=5)\n",
    "axes[2].set_xlabel('m (slope)')\n",
    "axes[2].set_ylabel('b (intercept)')\n",
    "axes[2].set_title('Path Through Parameter Space')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251a1991",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **1.4: \"Animated\" Gradient Descent**\n",
    "\n",
    "Let's create a step-by-step visualization to really understand how gradient descent works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecd540f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show gradient descent step by step\n",
    "def visualize_gd_steps(X, y, n_steps=10, learning_rate=0.05):\n",
    "    \"\"\"Create a multi-panel visualization of gradient descent steps.\"\"\"\n",
    "\n",
    "    # Initialize\n",
    "    np.random.seed(123)\n",
    "    m, b = 0.5, -2.0  # Start far from optimum\n",
    "\n",
    "    # Create figure\n",
    "    rows = np.ceil(n_steps / 5)\n",
    "    fig, axes = plt.subplots(int(rows), 5, figsize=(18, rows*4))\n",
    "    x_line = np.linspace(-5, 5, 100)\n",
    "\n",
    "    for step in range(n_steps):\n",
    "        if rows == 1:\n",
    "            ax = axes[step % 5]\n",
    "        else:\n",
    "            ax = axes[step // 5, step % 5]\n",
    "\n",
    "        # Plot data and current fit\n",
    "        ax.scatter(X, y, alpha=0.4, s=20)\n",
    "        ax.plot(x_line, m * x_line + b, 'r-', linewidth=2)\n",
    "        ax.plot(x_line, true_m * x_line + true_b, 'g--', linewidth=1, alpha=0.5)\n",
    "\n",
    "        # Compute and display info\n",
    "        mse = compute_mse(m, b, X, y)\n",
    "        dm, db = compute_gradients(m, b, X, y)\n",
    "\n",
    "        ax.set_title(f'Step {step}\\nm={m:.2f}, b={b:.2f}\\nMSE={mse:.2f}')\n",
    "        ax.set_xlim(-6, 6)\n",
    "        ax.set_ylim(-20, 25)\n",
    "\n",
    "        # Update parameters for next step\n",
    "        m = m - learning_rate * dm\n",
    "        b = b - learning_rate * db\n",
    "\n",
    "    plt.suptitle('Gradient Descent: Step by Step\\n(Red = Current Fit, Green Dashed = True Line)',\n",
    "                 fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_gd_steps(X, y, n_steps=10, learning_rate=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4642ee0",
   "metadata": {},
   "source": [
    "## **1.5 Connection to Scikit-Learn**\n",
    "\n",
    "When you call `model.fit()` in scikit-learn, this is (roughly) what happens behind the scenes!\n",
    "\n",
    "Different models use different optimization algorithms, but gradient descent is the foundation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca27637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare our result to scikit-learn\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
    "\n",
    "# Standard linear regression (uses closed-form solution, not gradient descent)\n",
    "lr = LinearRegression()\n",
    "lr.fit(X.reshape(-1, 1), y)\n",
    "print(f\"LinearRegression: m = {lr.coef_[0]:.4f}, b = {lr.intercept_:.4f}\")\n",
    "\n",
    "# SGDRegressor (uses stochastic gradient descent)\n",
    "sgd = SGDRegressor(max_iter=1000, tol=1e-4, random_state=42)\n",
    "sgd.fit(X.reshape(-1, 1), y)\n",
    "print(f\"SGDRegressor:     m = {sgd.coef_[0]:.4f}, b = {sgd.intercept_[0]:.4f}\")\n",
    "\n",
    "# Our implementation\n",
    "print(f\"Our GD:           m = {final_m:.4f}, b = {final_b:.4f}\")\n",
    "\n",
    "# True values\n",
    "print(f\"True:             m = {true_m}, b = {true_b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d821e888",
   "metadata": {},
   "source": [
    "## **1.6 Reflection Question**\n",
    "\n",
    "Today we saw how models learn through iterative improvement—taking small steps toward a goal. \n",
    "\n",
    "How does this relate to Philippians 3:14? In what ways is our own learning journey (academic, spiritual, or professional) similar to gradient descent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e701518f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **BREAK (10-15 minutes)**\n",
    "\n",
    "---\n",
    "\n",
    "## **2.1 Lab Exercises** (new notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4b7a1a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **3.1: Key Concepts Summary**\n",
    "\n",
    "### **3.1.1 What We Learned Today**\n",
    "\n",
    "1. **Loss Function**: Measures how wrong our model's predictions are (MSE for regression)\n",
    "\n",
    "2. **Gradient**: The direction and magnitude of steepest ascent\n",
    "   - Move in the **opposite** direction to descend\n",
    "\n",
    "3. **Gradient Descent Algorithm**:\n",
    "   1. Initialize parameters randomly\n",
    "   2. Compute gradient of loss\n",
    "   3. Update: params = params - learning_rate × gradient\n",
    "   4. Repeat until convergence\n",
    "\n",
    "4. **Learning Rate**: \n",
    "   - Too small → slow convergence\n",
    "   - Too large → oscillation/divergence\n",
    "   - Just right → efficient convergence\n",
    "\n",
    "5. **Convergence**: We stop when the loss stops decreasing significantly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84824fa6",
   "metadata": {},
   "source": [
    "## **3.2 Preview: Next Week**\n",
    "\n",
    "**Week 3: Linear Algebra for Data Science**\n",
    "- Dot products and why they matter\n",
    "- From DataFrames to Arrays\n",
    "- Matrix operations for efficiency\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py314",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
