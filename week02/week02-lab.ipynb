{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edeb77ff",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "\n",
    "# **Spring 2026 &mdash; CIS 3813<br>Advanced Data Science<br>(Introduction to Machine Learning)**\n",
    "### Week 2: How Models Learn: Gradient Descent in Action\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e21911",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Lab Instructions**\n",
    "\n",
    "**Due Date**: Monday, 09 February @ 6:00 PM (with grace period until Wednesday, 11 February @ 11:59 PM)\n",
    "\n",
    "In this lab, you will:\n",
    "1. Implement gradient descent from scratch\n",
    "2. Explore the effects of different learning rates\n",
    "3. Visualize the optimization process\n",
    "4. Apply gradient descent to a real dataset\n",
    "\n",
    "**AI Usage**: \n",
    "- You may use AI tools for this lab\n",
    "- **REQUIRED**: Include AI attribution using the format shown in the syllabus\n",
    "- For B/A level credit, include detailed attribution in markdown cells\n",
    "\n",
    "## **Grading**\n",
    "\n",
    "| Component | Points |\n",
    "|-----------|--------|\n",
    "| Exercise 1: Implementing Gradient Descent | 25 |\n",
    "| Exercise 2: Learning Rate Experiments | 25 |\n",
    "| Exercise 3: Visualizing Convergence | 25 |\n",
    "| Exercise 4: Application & Reflection | 15 |\n",
    "| In-Class Mastery Assessment (Week 3) | 10 |\n",
    "| **Total** | **100** |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c5f602",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **AI Assistance Declaration**\n",
    "\n",
    "**Tools used:** [e.g., ChatGPT-4 / GitHub Copilot / Claude / None]\n",
    "\n",
    "**Sections with AI help:** [e.g., \"Exercise 3: Pipeline Creation\"]\n",
    "\n",
    "**What I learned:** [Brief description of key concepts AI helped you understand]\n",
    "\n",
    "**What I did independently:** [Sections you completed without AI assistance]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b5b231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell first to import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "# Set up plotting defaults\n",
    "# plt.style.use('seaborn-v0_8-whitegrid') (Optional)\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8262c697",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **Exercise 1: Implementing Gradient Descent (25 points)**\n",
    "\n",
    "## **1.1 Generate Training Data (5 points)**\n",
    "\n",
    "Create a dataset for linear regression with the following specifications:\n",
    "- True relationship: $y = 2.5x - 1.5 + \\epsilon$\n",
    "- $\\epsilon$ is Gaussian noise with mean 0 and standard deviation 1.5\n",
    "- 100 data points\n",
    "- X values uniformly distributed between -3 and 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b4aea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Generate the training data\n",
    "# Hint: Use np.random.uniform() for X and np.random.normal() for noise\n",
    "\n",
    "true_m = 2.5\n",
    "true_b = -1.5\n",
    "n_samples = 100\n",
    "\n",
    "# YOUR CODE HERE\n",
    "X = None  # Replace with your code\n",
    "y = None  # Replace with your code\n",
    "\n",
    "# Visualize your data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X, y, alpha=0.6, label='Data points')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Generated Dataset')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc39e94e",
   "metadata": {},
   "source": [
    "## **1.2 Implement the Loss Function (5 points)**\n",
    "\n",
    "In the lecture, we used Mean Squared Error (MSE). For this lab, you will implement **Mean Absolute Error (MAE)** instead:\n",
    "\n",
    "$$\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - (mx_i + b)|$$\n",
    "\n",
    "**Why MAE?** Unlike MSE, MAE treats all errors equally (no squaring), making it more robust to outliers. This also means the gradients will be different. (Meaning you'll need to derive them yourself in Part 1.3!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744aa1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mae(m, b, X, y):\n",
    "    \"\"\"\n",
    "    Compute Mean Absolute Error.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    m : float\n",
    "        Slope parameter\n",
    "    b : float\n",
    "        Intercept parameter\n",
    "    X : numpy array\n",
    "        Input features\n",
    "    y : numpy array\n",
    "        Target values\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    float : Mean absolute error\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # Hint: Use np.abs() for absolute value\n",
    "    pass  # Replace with your implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270f8a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your MAE function\n",
    "# These should make sense: better parameters = lower MAE\n",
    "print(f\"MAE with true parameters (m=2.5, b=-1.5): {compute_mae(2.5, -1.5, X, y):.4f}\")\n",
    "print(f\"MAE with m=0, b=0: {compute_mae(0, 0, X, y):.4f}\")\n",
    "print(f\"MAE with m=5, b=5: {compute_mae(5, 5, X, y):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8ee4ad",
   "metadata": {},
   "source": [
    "## **1.3 Implement the Gradient Calculation (10 points)**\n",
    "\n",
    "Now you need to derive and implement the gradients for MAE. This is different from MSE!\n",
    "\n",
    "**Your task:** Derive the partial derivatives of MAE with respect to $m$ and $b$.\n",
    "\n",
    "**Hints:**\n",
    "- The derivative of $|u|$ with respect to $u$ is $\\text{sign}(u)$, where $\\text{sign}(u) = +1$ if $u > 0$, $-1$ if $u < 0$, and $0$ if $u = 0$\n",
    "- Use the chain rule\n",
    "- Let $e_i = y_i - (mx_i + b)$ be the error for point $i$\n",
    "- NumPy has `np.sign()` function\n",
    "\n",
    "**Write your derivation here (in comments or markdown):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79c1a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(m, b, X, y):\n",
    "    \"\"\"\n",
    "    Compute the gradients of MAE with respect to m and b.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    m : float\n",
    "        Current slope parameter\n",
    "    b : float\n",
    "        Current intercept parameter\n",
    "    X : numpy array\n",
    "        Input features\n",
    "    y : numpy array\n",
    "        Target values\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (dm, db) - gradients with respect to m and b\n",
    "\n",
    "    Hint: Use np.sign() to get the sign of the errors\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # Step 1: Compute predictions\n",
    "    # Step 2: Compute errors (y - predictions)\n",
    "    # Step 3: Compute sign of errors\n",
    "    # Step 4: Apply chain rule for dm and db\n",
    "    pass  # Replace with your implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb0e2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your gradient function\n",
    "# At the true parameters, gradients should be close to zero\n",
    "dm, db = compute_gradients(true_m, true_b, X, y)\n",
    "print(f\"Gradients at true parameters: dm = {dm:.4f}, db = {db:.4f}\")\n",
    "print(\"(These should be close to zero)\")\n",
    "\n",
    "# At poor parameters, gradients should point toward better values\n",
    "dm, db = compute_gradients(0, 0, X, y)\n",
    "print(f\"\\nGradients at m=0, b=0: dm = {dm:.4f}, db = {db:.4f}\")\n",
    "print(\"(dm should be negative, indicating we need to increase m)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c8e53a",
   "metadata": {},
   "source": [
    "## **1.4 Implement the Full Gradient Descent Algorithm (5 points)**\n",
    "\n",
    "Now put it all together! Implement gradient descent that:\n",
    "1. Initializes m and b to given starting values\n",
    "2. Iteratively updates parameters using the gradient\n",
    "3. Records the history of parameters and loss values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947607a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, m_init, b_init, learning_rate, n_iterations):\n",
    "    \"\"\"\n",
    "    Perform gradient descent to find optimal m and b.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : numpy array\n",
    "        Input features\n",
    "    y : numpy array\n",
    "        Target values\n",
    "    m_init : float\n",
    "        Initial slope value\n",
    "    b_init : float\n",
    "        Initial intercept value\n",
    "    learning_rate : float\n",
    "        Step size for gradient descent\n",
    "    n_iterations : int\n",
    "        Number of iterations to run\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (final_m, final_b, history)\n",
    "        final_m, final_b: optimized parameters\n",
    "        history: list of dicts with keys 'm', 'b', 'mse' for each iteration\n",
    "    \"\"\"\n",
    "    m = m_init\n",
    "    b = b_init\n",
    "    history = []\n",
    "\n",
    "    for i in range(n_iterations):\n",
    "        # YOUR CODE HERE\n",
    "        # 1. Compute gradients\n",
    "        # 2. Update m and b\n",
    "        # 3. Compute MAE and record in history (use 'mae' as key)\n",
    "        pass  # Replace with your implementation\n",
    "\n",
    "    return m, b, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c87b3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your gradient descent implementation\n",
    "final_m, final_b, history = gradient_descent(\n",
    "    X, y,\n",
    "    m_init=0.0,\n",
    "    b_init=0.0,\n",
    "    learning_rate=0.1,\n",
    "    n_iterations=100\n",
    ")\n",
    "\n",
    "print(f\"Starting: m=0.0, b=0.0\")\n",
    "print(f\"Final: m={final_m:.4f}, b={final_b:.4f}\")\n",
    "print(f\"True: m={true_m}, b={true_b}\")\n",
    "print(f\"\\nFinal MAE: {history[-1]['mae']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d46fedf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **Exercise 2: Learning Rate Experiments (25 points)**\n",
    "\n",
    "## **2.1 Experiment with Different Learning Rates (15 points)**\n",
    "\n",
    "Run gradient descent with the following learning rates:\n",
    "- 0.001 (very small)\n",
    "- 0.01 (small)\n",
    "- 0.1 (medium)\n",
    "- 0.5 (large)\n",
    "- 1.0 (very large)\n",
    "\n",
    "Use the same starting point (m=0, b=0) and 500 iterations for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96b3d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run experiments with different learning rates\n",
    "learning_rates = [0.001, 0.01, 0.1, 0.5, 1.0]\n",
    "results = {}\n",
    "\n",
    "for lr in learning_rates:\n",
    "    # YOUR CODE HERE\n",
    "    # Run gradient descent and store results\n",
    "    pass\n",
    "\n",
    "# Print final parameters for each learning rate\n",
    "for lr in learning_rates:\n",
    "    # YOUR CODE HERE\n",
    "    # Print results\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dc33cb",
   "metadata": {},
   "source": [
    "## **2.2 Visualize Convergence (10 points)**\n",
    "\n",
    "Create a plot showing MAE vs. iteration for each learning rate. Use a log scale for the y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1557363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create convergence plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# Plot MAE history for each learning rate\n",
    "\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('MAE (log scale)')\n",
    "plt.title('Convergence for Different Learning Rates')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e2342a",
   "metadata": {},
   "source": [
    "### **Question 2.2.1 (Answer in the markdown cell below)**\n",
    "\n",
    "Based on your experiments:\n",
    "1. Which learning rate converged fastest?\n",
    "2. What happened with the very large learning rate (1.0)?\n",
    "3. What is the tradeoff between using a small vs. large learning rate?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e68732",
   "metadata": {},
   "source": [
    "**Your Answer:**\n",
    "\n",
    "1. [Your answer here]\n",
    "\n",
    "2. [Your answer here]\n",
    "\n",
    "3. [Your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcd3f0e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **Exercise 3: Visualizing the Optimization Path (25 points)**\n",
    "\n",
    "## **3.1 Create a Loss Surface (10 points)**\n",
    "\n",
    "Create a contour plot showing the loss surface (MAE as a function of m and b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029bfeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create the loss surface\n",
    "# Create a grid of m and b values\n",
    "m_range = np.linspace(-1, 5, 100)\n",
    "b_range = np.linspace(-4, 2, 100)\n",
    "M, B = np.meshgrid(m_range, b_range)\n",
    "\n",
    "# Compute MAE for each (m, b) combination\n",
    "Z = np.zeros_like(M)\n",
    "# YOUR CODE HERE\n",
    "# Fill in Z with MAE values\n",
    "\n",
    "# Create contour plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "# YOUR CODE HERE\n",
    "# Create contour plot and mark the true minimum\n",
    "\n",
    "plt.xlabel('m (slope)')\n",
    "plt.ylabel('b (intercept)')\n",
    "plt.title('Loss Surface (MAE)')\n",
    "plt.colorbar(label='MAE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247d50d8",
   "metadata": {},
   "source": [
    "## **3.2 Plot Optimization Paths (15 points)**\n",
    "\n",
    "On the same contour plot, show the optimization path for three different starting points:\n",
    "- Start 1: (m=4, b=1)\n",
    "- Start 2: (m=-0.5, b=-3)\n",
    "- Start 3: (m=1, b=1)\n",
    "\n",
    "Use learning_rate=0.1 and 200 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a28a26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run gradient descent from each starting point\n",
    "starting_points = [\n",
    "    (4.0, 1.0, 'red', 'Start 1'),\n",
    "    (-0.5, -3.0, 'blue', 'Start 2'),\n",
    "    (1.0, 1.0, 'green', 'Start 3')\n",
    "]\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# Run gradient descent for each starting point and store histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a59c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create visualization with all paths\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Draw contour\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Draw paths for each starting point\n",
    "# YOUR CODE HERE\n",
    "# Mark starting points with circles, ending points with stars\n",
    "\n",
    "# Mark true minimum\n",
    "plt.scatter([true_m], [true_b], color='black', s=300, marker='X',\n",
    "            label='True Minimum', zorder=10)\n",
    "\n",
    "plt.xlabel('m (slope)')\n",
    "plt.ylabel('b (intercept)')\n",
    "plt.title('Gradient Descent Paths from Different Starting Points')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939ea726",
   "metadata": {},
   "source": [
    "### **Question 3.2.1 (Answer in the markdown cell below)**\n",
    "\n",
    "Do all three paths converge to (approximately) the same point? Why or why not? What does this tell us about the loss surface for linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66ea7dd",
   "metadata": {},
   "source": [
    "**Your Answer:**\n",
    "\n",
    "[Your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4dcc1f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **Exercise 4: Application & Reflection (15 points)**\n",
    "\n",
    "## **4.1 Apply to a Non-Linear Function (10 points)**\n",
    "\n",
    "Now let's apply gradient descent to minimize a different function:\n",
    "\n",
    "$$f(x) = x^4 - 3x^2 + 2$$\n",
    "\n",
    "This function has multiple local minima. Use calculus to find the derivative, then implement gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13e148c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    \"\"\"The function to minimize: f(x) = x^4 - 3x^2 + 2\"\"\"\n",
    "    return x**4 - 3*x**2 + 2\n",
    "\n",
    "def df(x):\n",
    "    \"\"\"Derivative of f(x). YOU NEED TO COMPUTE THIS.\"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass  # Replace with the derivative\n",
    "\n",
    "def gradient_descent_1d(start, learning_rate, n_iterations):\n",
    "    \"\"\"1D Gradient descent for f(x)\"\"\"\n",
    "    x = start\n",
    "    history = [(x, f(x))]\n",
    "\n",
    "    for _ in range(n_iterations):\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "\n",
    "    return x, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e91090d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the function first\n",
    "x_plot = np.linspace(-2.5, 2.5, 200)\n",
    "y_plot = f(x_plot)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_plot, y_plot, 'b-', linewidth=2, label='$f(x) = x^4 - 3x^2 + 2$')\n",
    "plt.axhline(y=0, color='k', linewidth=0.5)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title('Function with Multiple Minima')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c871d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run gradient descent from different starting points\n",
    "# Try starting from: x = -2, x = 0.1, x = 2\n",
    "starting_points_1d = [-2.0, 0.1, 2.0]\n",
    "learning_rate = 0.005\n",
    "n_iterations = 200\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# Run gradient descent from each starting point\n",
    "# Create a visualization showing all paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b695a22",
   "metadata": {},
   "source": [
    "### **Question 4.1.1**\n",
    "\n",
    "Did gradient descent find the same minimum from all starting points? Explain what happened."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3156e0",
   "metadata": {},
   "source": [
    "**Your Answer:**\n",
    "\n",
    "[Your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e1d572",
   "metadata": {},
   "source": [
    "## **4.2 Reflection: Connecting to Faith (5 points)**\n",
    "\n",
    "In the lecture, we connected gradient descent to Philippians 3:14: *\"I press on toward the goal for the prize of the upward call of God in Christ Jesus.\"*\n",
    "\n",
    "In Part 4.1, you saw that gradient descent can get \"stuck\" in local minima—places that seem like the lowest point but aren't the true global minimum.\n",
    "\n",
    "### **Question 4.2.1**\n",
    "\n",
    "How might this relate to our spiritual journey? Can you think of ways that people (or you personally) might settle for \"local minima\" in life rather than pressing on toward the ultimate goal? How can we avoid getting stuck?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147fdd79",
   "metadata": {},
   "source": [
    "**Your Reflection:**\n",
    "\n",
    "[Your thoughtful response here - aim for 3-5 sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ec144a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Submission Checklist**\n",
    "\n",
    "Before submitting, make sure you have:\n",
    "\n",
    "- [ ] Completed the AI Assistance Declaration at the top\n",
    "- [ ] Exercise 1: All functions implemented and tested\n",
    "- [ ] Exercise 2: Learning rate experiments complete with visualization and written answers\n",
    "- [ ] Exercise 3: Loss surface and optimization paths visualized with written answers\n",
    "- [ ] Exercise 4: Non-linear function optimization and reflection completed\n",
    "- [ ] All code cells run without errors\n",
    "- [ ] Restarted kernel and run all cells to verify everything works\n",
    "\n",
    "### **Submission Instructions**\n",
    "\n",
    "1. Save this notebook\n",
    "2. **Restart kernel and run all cells** (Kernel → Restart & Run All)\n",
    "3. Verify all outputs appear correctly (especially visualizations)\n",
    "4. Check that all written responses are complete\n",
    "5. Submit the `.ipynb` file to Canvas before Monday, 09 February @ 6:00 PM\n",
    "   - Grace period until Wednesday, 11 February @ 11:59 PM\n",
    "\n",
    "**Remember:** This notebook submission is worth 90% of your Week 1 Lab grade. The remaining 10% comes from next week's in-class mastery assessment.\n",
    "\n",
    "---\n",
    "\n",
    "## **Next Week Preview**\n",
    "\n",
    "**Mastery Assessment (Week 3)**: Be prepared to answer 1-2 questions about gradient descent without AI assistance. Focus on:\n",
    "- What is a gradient and what does it tell us?\n",
    "- What happens if the learning rate is too large or too small?\n",
    "- How do MAE and MSE gradients differ? (Hint: think about sign vs. magnitude)\n",
    "- What is the relationship between the loss function and model parameters?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py314",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
