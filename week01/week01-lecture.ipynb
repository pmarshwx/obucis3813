{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3573d1d",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "\n",
    "# **Spring 2026 &mdash; CIS 3813<br>Advanced Data Science<br>(Introduction to Machine Learning)**\n",
    "### Week 1: The Machine Learning Workflow\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7803af9",
   "metadata": {},
   "source": [
    "**Date:** 26 January 2026\n",
    "**Time:** 6:00–9:00 PM  \n",
    "**Instructor:** Dr. Patrick T. Marsh  \n",
    "**Course Verse:** \"He has shown you, O mortal, what is good. And what does the Lord require of you? To act justly and to love mercy and to walk humbly with your God.\"  &mdash; *Micah 6:8 (NIV)*\n",
    "\n",
    "---\n",
    "## **Welcome and Course Orientation**\n",
    "Below are some highlights from the course syllabus. You can find the full details including a course schedule on Canvas.  \n",
    "\n",
    "- **Class Time:** Mondays 6–9 PM\n",
    "- **Office Hours:** Mondays & Wednesdays 4:30–5:50 PM (and by appointment / virtual)\n",
    "- **Graded Components:** \n",
    "    - Labs/Homework (35%)\n",
    "    - Mini-Kaggle Competition (5%)\n",
    "    - Exams (10% each; 20% comulative)\n",
    "    - Final Project (40%)\n",
    "        - Final Project Check-in (5%)\n",
    "        - Final Project Report (25%)\n",
    "        - Final Project Presentation (10%) \n",
    "- **AI Use:** Allowed with attribution and full understanding; Please see the syllabus for the full AI policy\n",
    "- **Grading Rubric Summary:** (Details in the syllabus)\n",
    "    * **D/F**: Incomplete/Deficient Work\n",
    "    * **C**: Working Solution (Baseline Competence)\n",
    "    * **B**: Demonstrated Understanding\n",
    "    * **A**: Independent Mastery\n",
    "\n",
    "---\n",
    "## **Week 1 Learning Objectives**\n",
    "\n",
    "By the end of today's session, you will be able to:\n",
    "1. Describe the stages of the machine learning workflow\n",
    "2. Explain why data leakage occurs and how to prevent it (the Golden Rule)\n",
    "3. Define bias and variance in the context of machine learning\n",
    "4. Build a complete ML pipeline using scikit-learn\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## **Today's Outline**\n",
    "- Lecture\n",
    "    1. What is Machine Learning\n",
    "    2. The Machine Learning Workflow\n",
    "    3. How Models Fail\n",
    "    4. The Mathematical Perspective\n",
    "    5. Review: Functions & Slope from Calculus\n",
    "    6. Introduction to Scikit-Learn\n",
    "- Break (10-15 Minutes)\n",
    "- Lab (or Homework)\n",
    "- Review\n",
    "    1. Key Takeaways from Week 1\n",
    "    2. Common Pitfalls to Avoid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cda2572",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Opening Reflection**\n",
    "\n",
    "> *\"It is the glory of God to conceal a matter; to search out a matter is the glory of kings.\"*  \n",
    "> — Proverbs 25:2 (NIV)\n",
    "\n",
    "As we begin our journey into advanced data science, remember that uncovering patterns in data is a sacred task. God has embedded order and structure into creation, and our role as data scientists is to discover and interpret these patterns for the good of others. The work we do this semester—finding insights hidden in data—reflects the image of God in us."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1fc4b1",
   "metadata": {},
   "source": [
    "---\n",
    "## **1.1 What is Machine Learning?**\n",
    "\n",
    "Machine learning is the practice of using algorithms to parse data, learn from it, and make predictions or decisions. Unlike traditional programming where we explicitly code rules, in ML we let the algorithm discover the rules from the data.\n",
    "\n",
    "**Questions:**\n",
    "* How does a baby learn to identify a cat?\n",
    "* How does a scientist discover a new species?\n",
    "\n",
    "**Three Types of Machine Learning:**\n",
    "1. **Supervised Learning**: Learning from labeled data (aka with an \"Answer Key\")\n",
    "    - **Regression:** Predicting a continuous quantity\n",
    "    - **Classification:** Predicting a label\n",
    "2. **Unsupervised Learning**: Finding patterns in unlabeled data (aka without an \"Answer Key\")\n",
    "    - **Clustering:** Grouping like data\n",
    "    - **Dimensionality Reduction:** Compressing Data\n",
    "3. **Reinforcement Learning**: Learning through trial and error (rewards/punishments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d4e7b0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **1.2 The Machine Learning Workflow**\n",
    "\n",
    "Below is a commonly used Data Science workflow. We referenced it several times last semester. Understanding this workflow is crucial to becoming an effective data scientist.\n",
    "\n",
    "**The Complete Data Science Workflow:**\n",
    "<div style=\"text-align: center;\">\n",
    "\n",
    "**Problem $\\longleftrightarrow$ Data Acquistion $\\longleftrightarrow$ Cleaning/Preparation $\\longleftrightarrow$ Exploration/Visualization $\\longleftrightarrow$ Modeling/Inference $\\longleftrightarrow$ Evaluation $\\longleftrightarrow$ Communication/Deployment**\n",
    "\n",
    "</div>\n",
    "\n",
    "**Notice that the arrows point in both directions.** In practice the arrows are a bit more muddled. You can end up jumping from any point of the process to any other point of the process &mdash; multiple times &mdash; as you discover issues or new questions. \n",
    "\n",
    "Every machine learning project follows this same workflow. This semester we will focus on the *Modeling/Inference* and *Evaluation* steps. Specifically, we will expand the *Modeling/Inference* and *Evaluation* steps.\n",
    "\n",
    "**Expanded ML Workflow (our focus this semester):**\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "\n",
    "**... $\\longleftrightarrow$ [Data] Cleaning/Preparation $\\longleftrightarrow$ Model Selection $\\longleftrightarrow$ Data Splitting $\\longleftrightarrow$ Model Training $\\longleftrightarrow$ Model Evaluation $\\longleftrightarrow$ Model Refinement $\\longleftrightarrow$ ...**\n",
    "\n",
    "</div>\n",
    "\n",
    "This is essentially a \"zoom in\" on the middle portion of the complete workflow. Each step has purpose to avoid model \"memorization\" and allow actual \"learning\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3afb632",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **1.3 How Models Fail**\n",
    "\n",
    "### **The Exam Analogy**\n",
    "\n",
    "Think of your time in college. In a lot of your classes you are given a set of homework problems each week to help you learn the content for the course. Now, let's think of your final exam. If your final exam consists solely of the homework problems, you can earn a 100% on the exam simply by memorizing the answers to the homework problems. *There is no guarantee of learning!* If the final exam is similar, but not identical, to the homework problems, studying the homework problems should lead to a good final exam score. This demonstrates you learned the material. \n",
    "\n",
    "In the above scenario, the homework problems are analogous to a machine learning model's training dataset, whereas the final exam is analogous to the test dataset. If the final exam solely consists of the homework problems, the model should do extremely well. But the model has not demonstrated it's ability to learn. There is no telling how well it will do against questions it has not already seen before.\n",
    "\n",
    "This concept is summed up in the ***<u>Golden Rule of Data Science</u>***: **Never train on your test data**.\n",
    "\n",
    "### **What Happens When You Break the Golden Rule?**\n",
    "\n",
    "Let's see the danger in action:\n",
    "\n",
    "You are working for a biotech startup trying to develop a diagnostic test for a rare autoimmune disease. You have blood samples from 100 patients (50 sick; 50 healthy) and you run a gene sequencing panel that measures the expression levels of 50,000 genes for each patient. The company founder has asked you to build a model using these data to predict whether a patient has the autoimmune disease.\n",
    "\n",
    "You recognize that you cannot test all 50,000 gene expression levels due to performance reasons. Instead you select the top 10 gene expressesion levels that correlate most strongly to the disease. Next you train a classifier using only those 10 gene expression levels and evaluate your results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a88193e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# ==========================================\n",
    "# 0. GENERATE DATA (PURE NOISE)\n",
    "# ==========================================\n",
    "# 100 Patients, 50,000 Genes. Pure random numbers.\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "n_features = 50000\n",
    "n_best_features = 10\n",
    "\n",
    "X = np.random.normal(size=(n_samples, n_features))\n",
    "y = np.random.randint(0, 2, size=n_samples) # Random Sick (1) or Healthy (0)\n",
    "\n",
    "print(f\"Dataset: {n_samples} samples, {n_features} features (ALL NOISE)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# ==========================================\n",
    "# 1. THE WRONG WAY\n",
    "# ==========================================\n",
    "# CRIME: Selecting features BEFORE splitting\n",
    "selector = SelectKBest(f_classif, k=n_best_features)\n",
    "X_selected_leak = selector.fit_transform(X, y) # <--- The model sees the test answers here!\n",
    "\n",
    "X_train_l, X_test_l, y_train_l, y_test_l = train_test_split(\n",
    "    X_selected_leak, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "model_leak = LogisticRegression()\n",
    "model_leak.fit(X_train_l, y_train_l)\n",
    "acc_leak = accuracy_score(y_test_l, model_leak.predict(X_test_l))\n",
    "\n",
    "print(f\"LEAKY Approach:   {acc_leak*100:.1f}% Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcb553b",
   "metadata": {},
   "source": [
    "Congratulations! You just built a model that currectly identified the disease with 93.3% accurancy. It's time to celebrate!\n",
    "\n",
    "Or did you ...\n",
    "\n",
    "It turns out that when you deployed your model in a real hostpital, the accuracy drops to around 50%, which is no different than a random guess.\n",
    "\n",
    "#### **Why did your model fail?**\n",
    "\n",
    "Your results looked good, but they were artificially inflated! The reason is that with 50,000 gene expression levels and only 100 patients, probability theory dictates that *some* gene expression levels will correlate perfectly with the target variable purely by random chance. By selecting features on the whoel dataset, you cherry-picked these statistical coincidences for your model. This is known as **data leakage**. Data from the test dataset *leaked* into your training dataset. Your model didn't learn biology; it memorized the noise of those specific 100 patients.\n",
    "\n",
    "**Real-world consequences of data leakage:**\n",
    "- Your model performs great in testing but fails in production\n",
    "- A medical diagnosis model that seems 95% accurate but fails on real patients (our case)\n",
    "- A fraud detection system that looks perfect but misses actual fraud\n",
    "- A loan approval model that appears fair but makes biased decisions\n",
    "\n",
    "In all cases, you've lied to yourself about your model's true performance.\n",
    "\n",
    "**The correct approach**: Always split FIRST, then fit your preprocessing ONLY on training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491ab36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dataset: {n_samples} samples, {n_features} features (ALL NOISE)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# ==========================================\n",
    "# 1. THE CORRECT WAY\n",
    "# ==========================================\n",
    "# Split FIRST\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Step A: Fit the selector ONLY on Training data\n",
    "selector_manual = SelectKBest(f_classif, k=n_best_features)\n",
    "selector_manual.fit(X_train, y_train)\n",
    "\n",
    "# Step B: Transform both Train AND Test separately\n",
    "# RISK: It is very easy to accidentally type 'fit_transform' on X_test here!\n",
    "X_train_selected = selector_manual.transform(X_train)\n",
    "X_test_selected = selector_manual.transform(X_test)\n",
    "\n",
    "# Step C: Train model\n",
    "model_manual = LogisticRegression()\n",
    "model_manual.fit(X_train_selected, y_train)\n",
    "acc_manual = accuracy_score(y_test, model_manual.predict(X_test_selected))\n",
    "\n",
    "print(f\"LEAKY Approach:   {acc_leak*100:.1f}% Accuracy\")\n",
    "print(f\"MANUAL Approach:  {acc_manual*100:.1f}% Accuracy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f1c0ce",
   "metadata": {},
   "source": [
    "### **From Data Leakage to a Deeper Question: Why Do Models Fail?**\n",
    "\n",
    "The gene expression example teaches us about data leakage, but it also reveals something deeper: **even with perfect methodology, we can still build models that fail.**\n",
    "\n",
    "Let's imagine we fixed the leakage problem. We split our data first:\n",
    "- Training: 70 patients (35 sick, 35 healthy)\n",
    "- Testing: 30 patients (15 sick, 15 healthy)\n",
    "\n",
    "Now we select the top 10 genes using ONLY the training data, then evaluate on the test set.\n",
    "\n",
    "**We might still have a problem.** Why?\n",
    "\n",
    "### **The Statistical Reality: 50,000 Features vs. 70 Patients**\n",
    "\n",
    "Think about what we're asking the model to do:\n",
    "- Search through 50,000 genes\n",
    "- Find patterns in only 70 patients\n",
    "- Hope those patterns work on new patients\n",
    "\n",
    "This is like studying for an exam by:\n",
    "1. Reading a 50,000-page textbook\n",
    "2. Doing only 70 practice problems\n",
    "3. Hoping you can answer any question on the final exam\n",
    "\n",
    "**What's likely to happen?** You'll memorize specific details from those 70 practice problems instead of learning general principles. When the final exam asks slightly different questions, you're lost.\n",
    "\n",
    "#### **Two Ways Models Fail**\n",
    "\n",
    "This brings us to a fundamental insight in machine learning: **models can fail in two completely different ways.**\n",
    "\n",
    "##### **Failure Mode 1: Too Simple (Missing the Pattern)**\n",
    "\n",
    "Imagine you're trying to predict whether it will rain tomorrow. Your model is:\n",
    "- **\"It will rain if it rained today\"**\n",
    "\n",
    "This model is too simple! It ignores:\n",
    "- Temperature, humidity, pressure\n",
    "- Cloud cover, wind patterns\n",
    "- Seasonal trends\n",
    "\n",
    "**Problem**: The model is so simple it misses important patterns.  \n",
    "**Technical term**: This is called **high bias** or **underfitting**.  \n",
    "**Analogy**: A student who only memorized one formula and tries to use it for every problem.\n",
    "\n",
    "##### **Failure Mode 2: Too Complex (Memorizing Noise)**\n",
    "\n",
    "Now imagine your rain prediction model is:\n",
    "- **Track the exact temperature, humidity, pressure, wind speed, cloud cover, barometric pressure rate of change, lunar phase, and 42 other variables at 100 locations**\n",
    "- **Remember the exact weather pattern from every single day you've seen**\n",
    "- **If tomorrow looks even slightly like October 23, 2019, predict whatever happened on October 24, 2019**\n",
    "\n",
    "**Problem**: The model memorized specific instances instead of learning general patterns.  \n",
    "**Technical term**: This is called **high variance** or **overfitting**.  \n",
    "**Analogy**: A student who memorized all homework problems exactly but can't solve new problems.\n",
    "\n",
    "**Our gene expression model had Failure Mode 2**: With 50,000 genes and only 70 patients, it memorized which genes happened to correlate in those specific 70 people, not which genes actually cause the disease."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109f4c18",
   "metadata": {},
   "source": [
    "### **The Bias-Variance Tradeoff: The Central Challenge of Machine Learning**\n",
    "\n",
    "Here's the fundamental problem we face as data scientists:\n",
    "\n",
    "**We want models that generalize to new data, but:**\n",
    "- Simple models miss important patterns (high bias)\n",
    "- Complex models memorize noise (high variance)\n",
    "- We need to find the sweet spot in between\n",
    "\n",
    "This tension is called the **Bias-Variance Tradeoff**.\n",
    "\n",
    "### **Defining Bias and Variance**\n",
    "\n",
    "Before we visualize the tradeoff, we need to define what **bias** and **variance** actually mean in machine learning. These terms have specific technical meanings that are different from everyday usage.\n",
    "\n",
    "#### **What is Bias?**\n",
    "\n",
    "**Bias** measures how far off your model's **average predictions** are from the **true values**.\n",
    "\n",
    "Think of it like this:\n",
    "- You're throwing darts at a dartboard\n",
    "- **High bias** = Your darts consistently land away from the bullseye (systematically off-target)\n",
    "- You might hit the same wrong spot repeatedly\n",
    "- The problem is your **aim** is wrong, not that your throws are inconsistent\n",
    "\n",
    "**In machine learning:**\n",
    "- High bias means your model is **too simple** to capture the true relationship\n",
    "- It makes systematic errors because it's missing important patterns\n",
    "- Like using a straight line to fit data that curves\n",
    "\n",
    "**Example from gene expression:**\n",
    "- If we only use 2 genes to predict disease, we might have high bias\n",
    "- We're systematically wrong because we're ignoring 48 other important genes\n",
    "- Our model is too simple to capture the biological complexity\n",
    "\n",
    "#### **What is Variance?**\n",
    "\n",
    "**Variance** measures how much your model's predictions **change** when trained on **different datasets**.\n",
    "\n",
    "Back to the dartboard:\n",
    "- **High variance** = Your darts land all over the place (inconsistent)\n",
    "- Sometimes left, sometimes right, sometimes up, sometimes down\n",
    "- The problem is your **consistency**, not necessarily your average aim\n",
    "\n",
    "**In machine learning:**\n",
    "- High variance means your model is **too complex** and sensitive to training data specifics\n",
    "- Train on slightly different data → Get wildly different predictions\n",
    "- Like fitting a wiggly curve that passes through every single training point\n",
    "\n",
    "**Example from gene expression:**\n",
    "- With 50,000 genes and only 70 patients, we have high variance\n",
    "- Train on 70 different patients → Model picks completely different genes\n",
    "- Our model memorized noise specific to the training set\n",
    "### **Visual Intuition: The Dartboard Analogy**\n",
    "\n",
    "![Bias-Variance Tradeoff](./week01-lecture_files/bias-variance-dart-board.png)\n",
    "\n",
    "Source Image from https://blogs.alisterluiz.com/understanding-the-bias-variance-tradeoff-a-comprehensive-guide/\n",
    "\n",
    "**The goal:** Low bias AND low variance (top-left quadrant)  \n",
    "**The reality:** We usually have to trade one for the other\n",
    "\n",
    "Now that we understand bias and variance, let's revisit our examples:\n",
    "\n",
    "#### **Gene Expression Model (50,000 genes, 70 patients)**\n",
    "- **Bias:** LOW - Model is complex enough to fit any pattern\n",
    "- **Variance:** HIGH - Model changes dramatically with different training data\n",
    "- **Problem:** Overfitting (memorizing noise)\n",
    "- **Prediction:** Fails on new patients\n",
    "\n",
    "#### **\"Rain = Yesterday's Weather\" Model**\n",
    "- **Bias:** HIGH - Model is too simple, ignores important factors\n",
    "- **Variance:** LOW - Model is consistent (always uses same rule)\n",
    "- **Problem:** Underfitting (missing patterns)\n",
    "- **Prediction:** Consistently wrong\n",
    "\n",
    "#### **Homework Memorization**\n",
    "- **Bias:** LOW - You can recall every homework answer perfectly\n",
    "- **Variance:** HIGH - Only works for those exact problems\n",
    "- **Problem:** Overfitting (didn't learn concepts)\n",
    "- **Prediction:** Fails on exam (slightly different problems)\n",
    "\n",
    "#### **Not Studying**\n",
    "- **Bias:** HIGH - Don't understand the material\n",
    "- **Variance:** LOW - Consistently guessing wrong\n",
    "- **Problem:** Underfitting (didn't learn enough)\n",
    "- **Prediction:** Fails on exam (and homework)\n",
    "\n",
    "### **Now We're Ready for the Tradeoff**\n",
    "\n",
    "With bias and variance defined, we can understand the fundamental tradeoff:\n",
    "\n",
    "**As model complexity increases:**\n",
    "- Bias ↓ (model can fit more complex patterns)\n",
    "- Variance ↑ (model becomes more sensitive to training data)\n",
    "\n",
    "**The question:** Where do we stop? When is the model complex enough to capture patterns but not so complex that it memorizes noise?\n",
    "\n",
    "This is the **Bias-Variance Tradeoff**:\n",
    "\n",
    "\n",
    "#### **Visualizing the Tradeoff**\n",
    "\n",
    "| **Model Complexity →** | **Too Simple (Underfit)** | **Sweet Spot (Just Right)** | **Too Complex (Overfit)** |\n",
    "|---|---|---|---|\n",
    "| **Bias-Variance** | High Bias, Low Variance | Balanced Bias-Variance | Low Bias, High Variance |\n",
    "| **Pattern Recognition** | Misses patterns, ignores signal | Learns patterns, captures signal | Memorizes noise, captures noise |\n",
    "| **Training Performance** | Poor | Good | Perfect |\n",
    "| **Test Performance** | Poor | Good | Poor |\n",
    "| **Problem** | Model too simple to capture real relationships | Model complexity matches problem complexity | Model too complex, fits random noise |\n",
    "\n",
    "#### **Connecting Back to Our Examples (Again)**\n",
    "\n",
    "**Gene Expression Example:**\n",
    "- **50,000 features, 70 patients** → Way too complex → High variance → Overfitting\n",
    "- Model memorized random correlations specific to those 70 patients\n",
    "- Failed on new patients because those correlations don't generalize\n",
    "\n",
    "**Homework vs. Exam Analogy:**\n",
    "- **Memorizing answers** → High variance → You've overfit to the homework\n",
    "- **Not studying at all** → High bias → You underfit (too simple)\n",
    "- **Understanding concepts** → Balanced → You generalize to the exam\n",
    "\n",
    "**Rain Prediction:**\n",
    "- **\"Rain = Yesterday's weather\"** → High bias → Underfit\n",
    "- **Track 100 variables at 100 locations** → High variance → Overfit\n",
    "- **Temperature + humidity + pressure** → Balanced → Just right"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6b6320",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## **1.4 The Mathematical Perspective**\n",
    "\n",
    "For those interested in the theory, here's what's happening mathematically:\n",
    "\n",
    "**Total Error = Bias² + Variance + Irreducible Error**\n",
    "\n",
    "Where:\n",
    "- **Bias² = Error from wrong assumptions** (model too simple)\n",
    "- **Variance = Error from sensitivity to training data** (model too complex)\n",
    "- **Irreducible Error = Natural randomness** (we can't fix this)\n",
    "\n",
    "As we increase model complexity:\n",
    "- Bias ↓ (model can capture more patterns)\n",
    "- Variance ↑ (model becomes more sensitive to training data specifics)\n",
    "\n",
    "**The sweet spot**: Where total error is minimized.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acfa809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conceptual visualization (we'll code this properly later)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "complexity = np.linspace(0, 10, 100)\n",
    "bias_squared = 10 / (1 + complexity)  # Decreases with complexity\n",
    "variance = complexity ** 1.5 / 5       # Increases with complexity\n",
    "irreducible = np.ones_like(complexity) * 2\n",
    "\n",
    "total_error = bias_squared + variance + irreducible\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(complexity, bias_squared, label='Bias²', linewidth=2)\n",
    "plt.plot(complexity, variance, label='Variance', linewidth=2)\n",
    "plt.plot(complexity, total_error, label='Total Error', linewidth=3, color='purple')\n",
    "plt.axvline(x=complexity[np.argmin(total_error)], color='green',\n",
    "            linestyle='--', label='Optimal Complexity')\n",
    "plt.xlabel('Model Complexity →', fontsize=12)\n",
    "plt.ylabel('Error', fontsize=12)\n",
    "plt.title('The Bias-Variance Tradeoff', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d1e732",
   "metadata": {},
   "source": [
    "\n",
    "### **How Do We Find the Sweet Spot?**\n",
    "\n",
    "Great question! Throughout this semester, we'll learn strategies:\n",
    "\n",
    "#### **Strategy 1: Use More Training Data**\n",
    "- More data → Harder to overfit\n",
    "- Gene example: 50,000 genes would need 100,000+ patients\n",
    "- Not always possible (data is expensive!)\n",
    "\n",
    "#### **Strategy 2: Reduce Model Complexity**\n",
    "- Fewer features (feature selection done correctly)\n",
    "- Simpler algorithms\n",
    "- Gene example: Test only 100 most promising genes\n",
    "\n",
    "#### **Strategy 3: Regularization (Coming in Week 4!)**\n",
    "- Add penalty for complexity\n",
    "- Forces model to focus on strongest patterns\n",
    "- Ridge, Lasso, Elastic Net\n",
    "\n",
    "#### **Strategy 4: Cross-Validation (Week 5!)**\n",
    "- Test on multiple train/test splits\n",
    "- Get honest estimate of generalization\n",
    "- Tune complexity to minimize test error\n",
    "\n",
    "#### **Strategy 5: Ensemble Methods (Weeks 11-12!)**\n",
    "- Combine many models\n",
    "- Average out the variance\n",
    "- Random Forests, Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee92c42",
   "metadata": {},
   "source": [
    "### **Connecting to Our Faith Integration**\n",
    "\n",
    "Remember our opening verse:\n",
    "\n",
    "> *\"It is the glory of God to conceal a matter; to search out a matter is the glory of kings.\"*  \n",
    "> — Proverbs 25:2\n",
    "\n",
    "The Bias-Variance Tradeoff teaches us about **humility in our models**:\n",
    "\n",
    "**High Bias (Underfitting)** = **Intellectual laziness**\n",
    "- We assume the world is simpler than it is\n",
    "- We ignore important complexities\n",
    "- Like reducing humans to a single dimension\n",
    "\n",
    "**High Variance (Overfitting)** = **Intellectual pride**\n",
    "- We think we can model everything perfectly\n",
    "- We confuse memorization with understanding\n",
    "- Like claiming to know exactly why each person makes each choice\n",
    "\n",
    "**The Sweet Spot** = **Humble wisdom**\n",
    "- Recognize real patterns God has embedded in creation\n",
    "- Admit we cannot perfectly model every detail\n",
    "- Build models that serve others without claiming omniscience\n",
    "\n",
    "As Micah 6:8 reminds us to \"walk humbly,\" the Bias-Variance Tradeoff reminds us to model humbly—capturing what we can learn while admitting what we cannot.\n",
    "\n",
    "### **Questions for Discussion**\n",
    "\n",
    "1. Can you think of real-world decisions where you've seen \"underfitting\" (too simple thinking)?\n",
    "2. Can you think of situations where you've seen \"overfitting\" (over-interpreting limited data)?\n",
    "3. In the gene expression example, what would you do if you had to deploy a model with limited data?\n",
    "4. How does the Bias-Variance Tradeoff apply to other fields (medicine, law, theology)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d07c753",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **1.5 Review: Functions & Slope from Calculus**\n",
    "\n",
    "Before we dive into machine learning models, we need to review some fundamental concepts from calculus that underpin how models learn.\n",
    "\n",
    "### **Functions**\n",
    "\n",
    "A **function** is a relationship between inputs and outputs where each input maps to exactly one output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832dab76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: A simple linear function\n",
    "def f(x):\n",
    "    return 2 * x + 3\n",
    "\n",
    "# Test it\n",
    "print(f(0))   # Output: 3\n",
    "print(f(1))   # Output: 5\n",
    "print(f(5))   # Output: 13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e54756c",
   "metadata": {},
   "source": [
    "\n",
    "In machine learning, we're trying to find the function that best describes the relationship between our input features (X) and our target variable (y).\n",
    "\n",
    "### **Slope (The Derivative)**\n",
    "\n",
    "The **slope** tells us how much the output changes when we change the input. In calculus, we use the derivative to find the slope at any point.\n",
    "\n",
    "For a linear function: `y = mx + b`\n",
    "- `m` is the slope\n",
    "- `b` is the y-intercept\n",
    "\n",
    "**Why does slope matter in ML?**\n",
    "- Models learn by finding the slope (gradient) of the error\n",
    "- They adjust parameters to reduce error\n",
    "- This process is called **gradient descent** (we'll cover this in Week 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a64d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualizing slope\n",
    "x = np.linspace(-5, 5, 100)\n",
    "y = 2 * x + 3\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, y, 'b-', linewidth=2, label='y = 2x + 3')\n",
    "plt.axhline(y=0, color='k', linewidth=0.5)\n",
    "plt.axvline(x=0, color='k', linewidth=0.5)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('y', fontsize=12)\n",
    "plt.title('Linear Function: Slope = 2', fontsize=14)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# The slope is 2: for every 1 unit increase in x, y increases by 2 units"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40c741d",
   "metadata": {},
   "source": [
    "\n",
    "### **Key Calculus Concept: Rate of Change**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a25e10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approximate derivative (slope at a point)\n",
    "def approximate_derivative(f, x, h=0.0001):\n",
    "    \"\"\"\n",
    "    Calculate the approximate derivative of function f at point x\n",
    "    Using the definition: f'(x) ≈ [f(x+h) - f(x)] / h\n",
    "    \"\"\"\n",
    "    return (f(x + h) - f(x)) / h\n",
    "\n",
    "# Example function\n",
    "def f(x):\n",
    "    return x**2  # f(x) = x²\n",
    "\n",
    "# Find slope at x = 3\n",
    "# The derivative of x² is 2x, so at x=3, slope should be 6\n",
    "deltas = [1, 0.1, 0.01, 0.001, 0.0001, 1e-5, 1e-6]\n",
    "for delta in deltas:\n",
    "    slope = approximate_derivative(f, 3, delta)\n",
    "    print(f\"Approximate slope of x² at x=3 with h={delta}: {slope:.4f}\")\n",
    "slope_at_3 = approximate_derivative(f, 3, 1)\n",
    "print(f\"Actual slope (derivative = 2x): {2*3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e5fd96",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **1.6 Introduction to Scikit-Learn**\n",
    "\n",
    "**Scikit-learn** is Python's premier machine learning library. It provides:\n",
    "- Simple and efficient tools for data mining and data analysis\n",
    "- Accessible to everybody and reusable in various contexts\n",
    "- Built on NumPy, SciPy, and matplotlib\n",
    "- Open source, commercially usable (BSD license)\n",
    "\n",
    "### **Basic Scikit-Learn Workflow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6062d2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Scikit-learn follows a consistent API:\n",
    "# 1. Import the model\n",
    "# 2. Instantiate the model\n",
    "# 3. Fit the model to training data\n",
    "# 4. Predict on new data\n",
    "# 5. Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b727e304",
   "metadata": {},
   "source": [
    "### **Scikit-Learn Pipeline: A Practical Example**\n",
    "\n",
    "Let's work through a complete example using the California housing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb24f220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style for better-looking plots\n",
    "# sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2cdc91",
   "metadata": {},
   "source": [
    "#### **Step 1: Load and Explore Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d670983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the California Housing dataset\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "# Create a DataFrame for easier manipulation\n",
    "df = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
    "df['MedHouseValue'] = housing.target\n",
    "\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nDataset Description:\")\n",
    "print(df.describe())\n",
    "\n",
    "print(\"\\nFeature names:\")\n",
    "for i, name in enumerate(housing.feature_names):\n",
    "    print(f\"{i+1}. {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0749fd22",
   "metadata": {},
   "source": [
    "#### **Step 2: Visualize the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74091882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the relationship between features and target\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, col in enumerate(housing.feature_names):\n",
    "    axes[i].scatter(df[col], df['MedHouseValue'], alpha=0.3, s=1)\n",
    "    axes[i].set_xlabel(col)\n",
    "    axes[i].set_ylabel('Median House Value')\n",
    "    axes[i].set_title(f'{col} vs House Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76f515d",
   "metadata": {},
   "source": [
    "#### **Step 3: Prepare the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200abfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features (X) and target (y)\n",
    "X = df[housing.feature_names]\n",
    "y = df['MedHouseValue']\n",
    "\n",
    "# Split into training and testing sets (80/20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set size: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99eb2aed",
   "metadata": {},
   "source": [
    "#### **Step 4: Build a Pipeline (without Pipeline object first)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bfd9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Manual approach (step-by-step)\n",
    "print(\"=\" * 50)\n",
    "print(\"METHOD 1: MANUAL APPROACH\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Step 1: Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 2: Train the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Step 3: Make predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Step 4: Evaluate\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nManual Approach Results:\")\n",
    "print(f\"RMSE: ${rmse:.4f} (in hundreds of thousands)\")\n",
    "print(f\"R² Score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779dc239",
   "metadata": {},
   "source": [
    "#### **Step 5: Build a Pipeline (with Pipeline object)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba05852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Using Pipeline (RECOMMENDED)\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"METHOD 2: USING PIPELINE (RECOMMENDED)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create a pipeline that scales then applies linear regression\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "# Fit the entire pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions (scaling happens automatically!)\n",
    "y_pred_pipeline = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "mse_pipeline = mean_squared_error(y_test, y_pred_pipeline)\n",
    "rmse_pipeline = np.sqrt(mse_pipeline)\n",
    "r2_pipeline = r2_score(y_test, y_pred_pipeline)\n",
    "\n",
    "print(f\"\\nPipeline Results:\")\n",
    "print(f\"RMSE: ${rmse_pipeline:.4f} (in hundreds of thousands)\")\n",
    "print(f\"R² Score: {r2_pipeline:.4f}\")\n",
    "\n",
    "print(\"\\n✓ Results are identical! But Pipeline is cleaner and less error-prone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a630647d",
   "metadata": {},
   "source": [
    "#### **Step 6: Understand the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8eaafca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the linear regression model from the pipeline\n",
    "lr_model = pipeline.named_steps['regressor']\n",
    "\n",
    "# Show feature importance (coefficients)\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': housing.feature_names,\n",
    "    'Coefficient': lr_model.coef_\n",
    "}).sort_values('Coefficient', key=abs, ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importance (Absolute Coefficients):\")\n",
    "print(feature_importance)\n",
    "\n",
    "# Visualize coefficients\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance['Feature'], feature_importance['Coefficient'])\n",
    "plt.xlabel('Coefficient Value')\n",
    "plt.title('Feature Importance in Linear Regression Model')\n",
    "plt.axvline(x=0, color='black', linestyle='--', linewidth=0.8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4919f59a",
   "metadata": {},
   "source": [
    "#### **Step 7: Visualize Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f051afd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual vs predicted\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred_pipeline, alpha=0.5, s=10)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],\n",
    "         'r--', linewidth=2, label='Perfect Prediction')\n",
    "plt.xlabel('Actual House Value')\n",
    "plt.ylabel('Predicted House Value')\n",
    "plt.title('Actual vs Predicted House Values')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate and visualize residuals (prediction errors)\n",
    "residuals = y_test - y_pred_pipeline\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_pred_pipeline, residuals, alpha=0.5, s=10)\n",
    "plt.axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals (Actual - Predicted)')\n",
    "plt.title('Residual Plot')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb69915",
   "metadata": {},
   "source": [
    "### **Why Use Pipelines?**\n",
    "\n",
    "**Benefits of Scikit-Learn Pipelines:**\n",
    "\n",
    "1. **Cleaner Code**: All steps in one object\n",
    "2. **Prevents Data Leakage**: Ensures test data isn't seen during training\n",
    "3. **Easier Deployment**: Save one object instead of multiple\n",
    "4. **Reproducibility**: Same preprocessing steps every time\n",
    "5. **Cross-Validation Ready**: Works seamlessly with cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb12afa",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **BREAK (10-15 minutes)**\n",
    "\n",
    "---\n",
    "\n",
    "## **2.1 Lab Exercises** (new notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea4bafa",
   "metadata": {},
   "source": [
    "---\n",
    "## **3.1 Key Takeaways from Week 1**\n",
    "\n",
    "### **1. The Machine Learning Workflow is Your Roadmap**\n",
    "Every ML project follows these 6 essential steps:\n",
    "- [Data] Cleaning/Preparation ↔ Model Selection ↔ Data Splitting ↔ Model Training ↔ Model Evaluation ↔ Model Refinement\n",
    "- Don't skip steps! Each one is crucial\n",
    "- Arrows point both directions—iteration is normal and expected\n",
    "\n",
    "### **2. Calculus Concepts Are the Foundation**\n",
    "- Functions map inputs to outputs (what we're trying to learn)\n",
    "- Slope (derivative) tells us rate of change\n",
    "- Models use gradients to learn from errors (more on this next week!)\n",
    "\n",
    "### **3. Scikit-Learn Is the Standard ML Library in Python**\n",
    "- Consistent API across all models\n",
    "- Extensive documentation and community support\n",
    "- Built-in tools for preprocessing, modeling, and evaluation\n",
    "\n",
    "### **4. Pipelines Are Essential for Professional ML**\n",
    "- Prevent data leakage by ensuring proper train/test separation\n",
    "- Make code cleaner and more maintainable\n",
    "- Ensure reproducibility across different runs\n",
    "- Chain preprocessing and modeling steps together seamlessly\n",
    "\n",
    "### **5. Data Leakage Causes Overly Optimistic Performance Estimates**\n",
    "- Training on test data (or using test data in preprocessing) inflates metrics\n",
    "- Real-world performance will be worse than reported\n",
    "- **Golden Rule**: Always split FIRST, then preprocess\n",
    "\n",
    "### **6. Models Can Fail in Two Fundamentally Different Ways**\n",
    "Even with perfect methodology and no data leakage:\n",
    "- **Too simple** (high bias, underfitting): Misses important patterns\n",
    "- **Too complex** (high variance, overfitting): Memorizes noise instead of learning patterns\n",
    "\n",
    "### **7. The Bias-Variance Tradeoff Is the Central Challenge of ML**\n",
    "- As model complexity increases: bias ↓ but variance ↑\n",
    "- Finding the right complexity is both an art and a science\n",
    "- This semester, we'll learn techniques to navigate this tradeoff (regularization, cross-validation, ensemble methods)\n",
    "\n",
    "### 8. **Linear Regression Is Your Baseline**\n",
    "- Simple but powerful first model\n",
    "- Easy to interpret (feature coefficients show importance)\n",
    "- Great starting point for any regression problem\n",
    "- Provides a benchmark for more complex models\n",
    "\n",
    "**Next week**: We'll see how models actually learn through **gradient descent**, which will help us understand how to control model complexity during training and why choosing the right learning rate matters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bf066d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **3.2 Common Pitfalls to Avoid**\n",
    "\n",
    "### **Pitfall 1: Data Leakage**\n",
    "```python\n",
    "# BAD\n",
    "X_scaled = scaler.fit_transform(X)  # Sees test data!\n",
    "X_train, X_test = train_test_split(X_scaled)\n",
    "```\n",
    "\n",
    "#### **Solution: Use Pipeline or fit only on training data**\n",
    "```python\n",
    "# GOOD\n",
    "X_train, X_test = train_test_split(X)\n",
    "pipeline = Pipeline([('scaler', StandardScaler()), ('model', LinearRegression())])\n",
    "pipeline.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "### **Pitfall 2: Not understanding what metrics mean**\n",
    "- Don't just report numbers—interpret them!\n",
    "- Consider the context of your problem\n",
    "- RMSE of 0.5 means prediction is off by $50,000 for this dataset\n",
    "\n",
    "### **Pitfall 3: Overfitting to your test set**\n",
    "- Test set is sacred—use it only once at the end\n",
    "- Don't repeatedly adjust your model based on test performance\n",
    "- Use cross-validation instead (coming in Week 5!)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py314",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
