{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3573d1d",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "\n",
    "# **Spring 2026 &mdash; CIS 3813<br>Advanced Data Science<br>(Introduction to Machine Learning)**\n",
    "### Week 4: Multiple Linear Regression & Regularization\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7803af9",
   "metadata": {},
   "source": [
    "**Date:** 16 February 2026\n",
    "**Time:** 6:00–9:00 PM  \n",
    "**Instructor:** Dr. Patrick T. Marsh  \n",
    "**Course Verse:** \"He has shown you, O mortal, what is good. And what does the Lord require of you? To act justly and to love mercy and to walk humbly with your God.\"  &mdash; *Micah 6:8 (NIV)*\n",
    "\n",
    "---\n",
    "\n",
    "## **Week 4 Learning Objectives**\n",
    "\n",
    "By the end of this lecture, you will be able to:\n",
    "\n",
    "1. Contrast simple linear regression with multiple linear regression and represent the latter using matrix notation.\n",
    "2. Define regularization and explain how it addresses the risks of overfitting and high model complexity.\n",
    "3. Compare and contrast L1 (Lasso) and L2 (Ridge) regularization penalties and their effects on model coefficients.\n",
    "4. Articulate why feature scaling is a mandatory prerequisite for effective regularization.\n",
    "5. Implement and evaluate regularized models using Scikit-Learn.\n",
    "\n",
    "---\n",
    "\n",
    "## **Today's Outline**\n",
    "- Lecture  \n",
    "    1. Review of Last Week\n",
    "    2. From Simple to Multiple Linear Regression\n",
    "    3. The Idea Behind Regularization\n",
    "    4. Feature Scaling &mdash; A CRITICAL Prerequisite\n",
    "    5. Ridge Regression (L2) in Action\n",
    "    6. Lasso Regression (L1) in Action\n",
    "    7. Summary: When to Use What\n",
    "- Break (10-15 Minutes)\n",
    "- Lab (or Homework)\n",
    "- Review\n",
    "    1. Review & Wrap-Up\n",
    "    2. Coming Up\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cda2572",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Opening Reflection**\n",
    "\n",
    "> *\"For which of you, desiring to build a tower, does not first sit down and count the cost, whether he has enough to complete it?\"*  \n",
    "> — **Luke 14:28 (ESV)**\n",
    "\n",
    "Just as a wise builder counts the cost before construction, a wise data scientist must carefully **count the features** before building a model. Tonight we learn that more features aren't always better — and that sometimes the most powerful thing a model can do is learn to *ignore* irrelevant information. Regularization teaches our models the discipline of simplicity, a principle that echoes the biblical call to focus on what truly matters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5122595c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **1.1 Review of Last Week: Gradient Descent Foundations**\n",
    "\n",
    "Last week, we established how a model \"learns\" by adjusting its parameters to minimize error. Before we scale up to multiple features and regularization today, remember these core concepts:\n",
    "\n",
    "* **The Gradient:** This is the slope of the loss function relative to a parameter. It provides two vital pieces of information: the **direction** to move (opposite the sign of the gradient) and the **steepness** of the current loss.\n",
    "* **The Update Rule:** We iteratively improve the model by subtracting the scaled gradient from our current weight: $w = w - (\\alpha \\times \\text{gradient})$.\n",
    "* **Learning Rate ($\\alpha$):** This controls our \"step size\". \n",
    "    * If **$\\alpha$ is too large**, we may overshoot the minimum and cause the loss to oscillate or increase.\n",
    "    * If **$\\alpha$ is too small**, the model learns too slowly to be practical.\n",
    "* **Mean Squared Error (MSE):** This loss function summarizes the model's total error into a single number that gradient descent works to minimize.\n",
    "* **From Single to Multiple:** Last week focused on a single weight and feature. We concluded by introducing the **dot product** ($\\mathbf{w} \\cdot \\mathbf{x} + b$), which allows us to scale these updates to hundreds of features simultaneously using vectorized operations.\n",
    "\n",
    "While last week focused on finding the right parameters for a simple model, this week we address what happens when we have too many features (leading to overfitting) and how we use L1 and L2 regularization as a \"complexity tax\" to keep our models simple and robust."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d782c354",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **1.2 From Simple to Multiple Linear Regression**\n",
    "\n",
    "### **1.2.1 Quick Recap: Simple Linear Regression**\n",
    "\n",
    "In Weeks 1–3, we worked with models that had **one feature** predicting **one target**:\n",
    "\n",
    "$$\\hat{y} = w_0 + w_1 x$$\n",
    "\n",
    "But real-world problems almost always involve **multiple features**. Think about predicting house prices — you wouldn't use *just* square footage. You'd also want bedrooms, bathrooms, neighborhood, age, etc.\n",
    "\n",
    "### **1.2.2 Multiple Linear Regression**\n",
    "\n",
    "With $p$ features, our model becomes:\n",
    "\n",
    "$$\\hat{y} = w_0 + w_1 x_1 + w_2 x_2 + \\cdots + w_p x_p$$\n",
    "\n",
    "Or in **matrix notation** (connecting to last week's linear algebra!):\n",
    "\n",
    "$$\\hat{\\mathbf{y}} = \\mathbf{X} \\mathbf{w}$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{X}$ is our feature matrix (with a column of 1s for the intercept)\n",
    "- $\\mathbf{w}$ is our weight/coefficient vector\n",
    "- The dot product we learned last week is exactly how predictions are computed!\n",
    "\n",
    "### **1.2.3 The Objective: Minimize the Cost Function**\n",
    "\n",
    "We still minimize the **Mean Squared Error (MSE)**, also called the Residual Sum of Squares (RSS):\n",
    "\n",
    "$$\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "Let's see this in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38edc8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports for tonight's lecture\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plots below may contain warnings. Uncomment these two lines to suppress them.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set a consistent style for our plots\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "print(\"All imports successful! Ready for Week 4.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fa291e",
   "metadata": {},
   "source": [
    "### **1.2.4 Loading a Real Dataset: California Housing**\n",
    "\n",
    "We'll work with the **California Housing dataset**, a classic ML dataset with 8 features predicting median house value in California districts. (We used this in Week 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebedbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the California Housing dataset\n",
    "housing = fetch_california_housing(as_frame=True)\n",
    "df = housing.frame\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nFeatures: {list(housing.feature_names)}\")\n",
    "print(f\"Target: {housing.target_names}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93979fa2",
   "metadata": {},
   "source": [
    "Let's understand what each feature represents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2405ff99",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(housing.DESCR[:1500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a334e76e",
   "metadata": {},
   "source": [
    "Let's look at some summary statistics. Pay close attention to the scales of the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63112fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ba530a",
   "metadata": {},
   "source": [
    "#### **Key Observation: Features Have Very Different Scales!**\n",
    "\n",
    "Look at the ranges:\n",
    "- `MedInc` (median income): roughly 0.5 to 15\n",
    "- `Population`: roughly 3 to 35,682\n",
    "- `AveRooms`: roughly 0.8 to 141\n",
    "\n",
    "**This matters for regularization!** We'll come back to this when we discuss why **feature scaling** is essential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d8a1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "X = df.drop('MedHouseVal', axis=1)\n",
    "y = df['MedHouseVal']\n",
    "\n",
    "# Train/test split (80/20) — remember this from Week 1!\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set:     {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef7a4a8",
   "metadata": {},
   "source": [
    "### **1.2.5 Fitting a Standard Multiple Linear Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f52a128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a standard (OLS) linear regression with ALL features\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = lr.predict(X_train)\n",
    "y_test_pred = lr.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(\"=== Standard Linear Regression ===\")\n",
    "print(f\"Train MSE: {mean_squared_error(y_train, y_train_pred):.4f}\")\n",
    "print(f\"Test  MSE: {mean_squared_error(y_test, y_test_pred):.4f}\")\n",
    "print(f\"Train R²:  {r2_score(y_train, y_train_pred):.4f}\")\n",
    "print(f\"Test  R²:  {r2_score(y_test, y_test_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3e517e",
   "metadata": {},
   "source": [
    "Let's exame the learned coefficients ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3d3a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Coefficient': lr.coef_\n",
    "}).sort_values('Coefficient', key=abs, ascending=False)\n",
    "\n",
    "print(f\"Intercept: {lr.intercept_:.4f}\\n\")\n",
    "print(\"Coefficients (sorted by absolute value):\")\n",
    "print(coef_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1bc545",
   "metadata": {},
   "source": [
    "Let's visualize the coefficients ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e07eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the coefficients\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "colors = ['#2ecc71' if c > 0 else '#e74c3c' for c in coef_df['Coefficient']]\n",
    "ax.barh(coef_df['Feature'], coef_df['Coefficient'], color=colors)\n",
    "ax.set_xlabel('Coefficient Value')\n",
    "ax.set_title('Standard Linear Regression Coefficients\\n(Green = Positive, Red = Negative)')\n",
    "ax.axvline(x=0, color='black', linewidth=0.8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cba3a2",
   "metadata": {},
   "source": [
    "### **1.2.6 The Problem: Can We Trust These Coefficients?**\n",
    "\n",
    "The coefficients above are **unscaled**. A coefficient of -0.007 for `Population` doesn't mean population is unimportant — it means a 1-unit change in population (1 person) changes the prediction by -0.007. But population ranges from 3 to 35,682!\n",
    "\n",
    "**Two major problems with standard linear regression:**\n",
    "\n",
    "1. **Interpretability with different scales:** Coefficients can't be directly compared across features\n",
    "2. **Overfitting risk:** With many features, the model can memorize training noise\n",
    "\n",
    "This is where **regularization** comes in!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c77a72",
   "metadata": {},
   "source": [
    "---\n",
    "## **1.3 The Idea Behind Regularization**\n",
    "\n",
    "Imagine a traveler going on a three-day trip to the mountains. He or she isn't sure what the weather will be, so the traveler decides to pack for every possible scenario. He or she packs a parka, a swimsuit, hiking boots, formal shoes, a tuxedo, three umbrellas, and a portable espresso machine.\n",
    "\n",
    "Now, imagine the traveler gets to the airport and find out the airline has a new policy: **Every pound packed costs $10.** \n",
    "\n",
    "Suddenly, the goal isn't just to be 'perfectly prepared'; the goal is to be as prepared as possible for the lowest cost. Travelers have to weigh the 'Accuracy' (having the right outfit) against the 'Penalty' (the baggage weight fee).\n",
    "\n",
    "Now, there are two different ways to approach this new \"cost\" problem. \n",
    "\n",
    "**Traveler A (Ridge / L2)**  \n",
    "This traveler decides to bring everything, but in *miniature form*. Instead of a full bottle of shampoo, he or she brings a travel-sized one. Instead of heavy boots, he or she bring ultralight sneakers. He or she brings the tuxedo/formal dress, but it's made of paper-thin fabric.\n",
    "\n",
    "**Traveler B (Lasso / L1)**  \n",
    "This travler is ruthless. He or she looks at the tuxedo/formal dress and says, \"I'm not paying $10 to carry that.\" *Toss.* He or she looks at the three umbrellas and keeps only one. *Toss, toss.* By the time the traveler reaches the scale, he or she only has a coat, a pair of boots, and a shirt. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ae9678",
   "metadata": {},
   "source": [
    "### **1.3.1 What Is Regularization?**\n",
    "\n",
    "Regularization adds a **penalty term** to the cost function that discourages the model from making coefficients too large. The intuition: a model with huge coefficients is probably overfitting to noise.\n",
    "\n",
    "Standard linear regression minimizes:\n",
    "\n",
    "$$\\text{Cost}_{OLS} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "Regularized regression minimizes:\n",
    "\n",
    "$$\\text{Cost}_{regularized} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\alpha \\cdot \\text{Penalty}(\\mathbf{w})$$\n",
    "\n",
    "Where $\\alpha$ (alpha) controls **how much** we penalize large coefficients.\n",
    "\n",
    "### **1.3.2 The Two Main Types of Regularization**\n",
    "\n",
    "| Method | Penalty Term | Effect | Scikit-Learn |\n",
    "|--------|-------------|--------|-------------|\n",
    "| **Ridge (L2)** | $\\alpha \\sum_{j=1}^{p} w_j^2$ | Shrinks coefficients toward zero | `Ridge()` |\n",
    "| **Lasso (L1)** | $\\alpha \\sum_{j=1}^{p} \\|w_j\\|$ | Shrinks AND can set coefficients to exactly zero | `Lasso()` |\n",
    "\n",
    "### **1.3.3 Back to Our Traveler Analogy**\n",
    "\n",
    "In our traveler analogy, the original packing strategy is analogous to **ordinary least squares** regression. On paper, the traveler is perfectly prepared for every thing the weather might throw. But this is **overfitting**. The suitcase is exploding, it weighs 100 pounds, and he or she will spend the whole trip digging through junk not needed.\n",
    "\n",
    "The introduction of a *penalty* for the weight of the bag is known as **regularization**.\n",
    "\n",
    "The packing equation is changed from making sure the traveler is not unprepared (error) to \n",
    "\n",
    "$$ \\text{Total Cost} = \\text{Unpreparedness } (error) + \\text{Baggage Fees } (Penalty) $$\n",
    "\n",
    "**Traveler A's** approach is analogous to Ridge / L2 Regularization. This travler still has every item, but each one has been shrunk to save weight. Very little (nothing) is left behind, but nothing is \"full-strength\". This is **Ridge Regression**—it keeps all the variables but minimizes their impact.\"\n",
    "\n",
    "**Traveler B's** approach is analogous to Lasso / L1 Regularization. This traveler  looked at the cost of each item and decided that some items simply weren't worth carrying. He or she removed the unimportant items. This traveler has performed **Feature Selection** by looking at the cost and deciding that some items simply aren't worth carrying. He or she essentially **zeroed out** the unimportant items. This is **Lasso Regression**.\"\n",
    "\n",
    "### **1.3.4 Intuition: Why L1 Can Zero Out Coefficients but L2 Cannot**\n",
    "\n",
    "This is one of the most important conceptual differences in ML. Let's visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac7d0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup the figure\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 6))\n",
    "theta = np.linspace(0, 2 * np.pi, 200)\n",
    "\n",
    "# Define the \"Ideal\" OLS solution (center of RSS ellipses)\n",
    "ols_x, ols_y = 1.3, 0.6\n",
    "\n",
    "# Define the shared RSS contour parameters\n",
    "# This represents a steep vertical ellipse\n",
    "rss_radii = np.arange(0.3, 3.3, 0.3)\n",
    "x_stretch = 1.0\n",
    "y_stretch = 2.0\n",
    "\n",
    "# RSS Contours: Using the vertical ellipse from Lasso subplot\n",
    "for r in rss_radii:\n",
    "    ellipse_x = ols_x + r * x_stretch * np.cos(theta)\n",
    "    ellipse_y = ols_y + r * y_stretch * np.sin(theta)\n",
    "    ax.plot(ellipse_x, ellipse_y, 'r--', alpha=0.4)\n",
    "\n",
    "# --- LEFT PLOT: RIDGE (L2) ---\n",
    "# Penalty: w1^2 + w2^2 <= 1 (A circle)\n",
    "axes[0].plot(np.cos(theta), np.sin(theta), 'b-', linewidth=3, label='L2 Penalty: $w_1^2 + w_2^2 \\leq t$')\n",
    "axes[0].fill(np.cos(theta), np.sin(theta), color='blue', alpha=0.1)\n",
    "\n",
    "# Ridge solution: Intersection with the circle (both weights > 0)\n",
    "# Note: For this specific geometry, the tangent point is roughly at (0.94, 0.34)\n",
    "ridge_sol_x, ridge_sol_y = 0.94, 0.34\n",
    "axes[0].plot(ridge_sol_x, ridge_sol_y, marker='o', color='#7DF9FF', markersize=10, label='Ridge Solution')\n",
    "axes[0].annotate('L2 Tangent point\\n($w_1$, $w_2$ both > 0)', xy=(ridge_sol_x, ridge_sol_y),\n",
    "                 xytext=(-0.9, -1.1), fontsize=11, fontweight='bold', ha='center', va='top',\n",
    "                 arrowprops=dict(arrowstyle='->', color=\"black\", lw=4))\n",
    "axes[0].set_title('Ridge Regression (L2)', fontsize=15)\n",
    "\n",
    "\n",
    "# --- RIGHT PLOT: LASSO (L1) ---\n",
    "# Penalty: |w1| + |w2| <= 1 (A diamond)\n",
    "diamond_x, diamond_y = [1, 0, -1, 0, 1], [0, 1, 0, -1, 0]\n",
    "axes[1].plot(diamond_x, diamond_y, 'g-', linewidth=3, label='L1 Penalty: $|w_1| + |w_2| \\leq t$')\n",
    "axes[1].fill(diamond_x, diamond_y, color='green', alpha=0.1)\n",
    "\n",
    "# Lasso solution: Hits the corner at (1, 0)\n",
    "lasso_sol_x, lasso_sol_y = 1.0, 0.0\n",
    "axes[1].plot(lasso_sol_x, lasso_sol_y, marker='o', color='#39FF14', markersize=10, label='Lasso Solution')\n",
    "axes[1].annotate('L1 Hits the Corner!\\n($w_2$ is exactly 0)', xy=(lasso_sol_x, lasso_sol_y),\n",
    "                 xytext=(1.2, -1.), fontsize=11, fontweight='bold', ha='center', va='top',\n",
    "                 arrowprops=dict(arrowstyle='->', color='black', lw=4))\n",
    "axes[1].set_title('Lasso Regression (L1)', fontsize=15)\n",
    "\n",
    "\n",
    "# --- COMMON AXES FORMATTING ---\n",
    "for ax in axes:\n",
    "\n",
    "    # RSS Contours: Using the vertical ellipse from Lasso subplot\n",
    "    for r in rss_radii:\n",
    "        ellipse_x = ols_x + r * x_stretch * np.cos(theta)\n",
    "        ellipse_y = ols_y + r * y_stretch * np.sin(theta)\n",
    "        ax.plot(ellipse_x, ellipse_y, 'r--', alpha=0.4)\n",
    "\n",
    "    # Plot the OLS minimum as a big, bold X at the center of the ellipses\n",
    "    ax.plot(ols_x, ols_y, 'x', color='#FF0000', markersize=12,\n",
    "            markeredgewidth=3, label='OLS Minimum')\n",
    "\n",
    "    ax.set_xlabel('$w_1$', fontsize=18)\n",
    "    ax.set_ylabel('$w_2$', fontsize=18)\n",
    "    ax.set_xlim(-1.5, 2.0)\n",
    "    ax.set_ylim(-1.5, 2.0)\n",
    "    ax.axhline(y=0, color='black', lw=1); ax.axvline(x=0, color='black', lw=1)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae87dfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup the figure\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "theta = np.linspace(0, 2 * np.pi, 200)\n",
    "\n",
    "# Define the \"Ideal\" OLS solution (center of RSS ellipses)\n",
    "ols_x, ols_y = 1.3, 0.6\n",
    "\n",
    "# Plot the OLS minimum as a big, bold X at the center of the ellipses\n",
    "ax.plot(ols_x, ols_y, 'x', color='#FF0000', markersize=12,\n",
    "        markeredgewidth=3, label='OLS Minimum')\n",
    "\n",
    "# Define the shared RSS contour parameters\n",
    "# This represents a steep vertical ellipse\n",
    "rss_radii = np.arange(0.3, 3.3, 0.3)\n",
    "x_stretch = 1.0\n",
    "y_stretch = 2.0\n",
    "\n",
    "# RSS Contours: Using the vertical ellipse from Lasso subplot\n",
    "for r in rss_radii:\n",
    "    ellipse_x = ols_x + r * x_stretch * np.cos(theta)\n",
    "    ellipse_y = ols_y + r * y_stretch * np.sin(theta)\n",
    "    ax.plot(ellipse_x, ellipse_y, 'r--', alpha=0.4)\n",
    "\n",
    "# --- RIDGE (L2) ---\n",
    "# Penalty: w1^2 + w2^2 <= 1 (A circle)\n",
    "ax.plot(np.cos(theta), np.sin(theta), 'b-', linewidth=3, label='L2 Penalty: $w_1^2 + w_2^2 \\leq t$')\n",
    "ax.fill(np.cos(theta), np.sin(theta), color='blue', alpha=0.1)\n",
    "\n",
    "# Ridge solution: Intersection with the circle (both weights > 0)\n",
    "# Note: For this specific geometry, the tangent point is roughly at (0.94, 0.34)\n",
    "ridge_sol_x, ridge_sol_y = 0.94, 0.34\n",
    "ax.plot(ridge_sol_x, ridge_sol_y, marker='o', color='#7DF9FF', markersize=10, label='Ridge Solution')\n",
    "ax.annotate('L2 Tangent point\\n($w_1$, $w_2$ both > 0)', xy=(ridge_sol_x, ridge_sol_y),\n",
    "                 xytext=(-1, -1.), fontsize=11, fontweight='bold', ha='center', va='top',\n",
    "                 arrowprops=dict(arrowstyle='->', color=\"black\", lw=4))\n",
    "\n",
    "ax.set_title('Ridge Regression (L2)\\nSmooth Circular Constraint', fontsize=15)\n",
    "ax.set_xlim(-1.5, 2.5); ax.set_ylim(-1.5, 2.0)\n",
    "ax.axhline(y=0, color='black', lw=1); ax.axvline(x=0, color='black', lw=1)\n",
    "ax.set_aspect('equal'); ax.legend()\n",
    "\n",
    "# --- LASSO (L1) ---\n",
    "# Penalty: |w1| + |w2| <= 1 (A diamond)\n",
    "diamond_x, diamond_y = [1, 0, -1, 0, 1], [0, 1, 0, -1, 0]\n",
    "ax.plot(diamond_x, diamond_y, 'g-', linewidth=3, label='L1 Penalty: $|w_1| + |w_2| \\leq t$')\n",
    "ax.fill(diamond_x, diamond_y, color='green', alpha=0.1)\n",
    "\n",
    "# Lasso solution: Hits the corner at (1, 0)\n",
    "lasso_sol_x, lasso_sol_y = 1.0, 0.0\n",
    "ax.plot(lasso_sol_x, lasso_sol_y, marker='o', color='#39FF14', markersize=10, label='Lasso Solution')\n",
    "ax.annotate('L1 Hits the Corner!\\n($w_2$ is exactly 0)', xy=(lasso_sol_x, lasso_sol_y),\n",
    "                 xytext=(1.2, -1.), fontsize=11, fontweight='bold', ha='center', va='top',\n",
    "                 arrowprops=dict(arrowstyle='->', color='black', lw=4))\n",
    "\n",
    "ax.set_xlabel('$w_1$', fontsize=18)\n",
    "ax.set_ylabel('$w_2$', fontsize=18)\n",
    "ax.set_title('Comparison of L2 vs L1', fontsize=18)\n",
    "ax.set_xlim(-1.5, 2.0)\n",
    "ax.set_ylim(-1.5, 2.0)\n",
    "ax.axhline(y=0, color='black', lw=1); ax.axvline(x=0, color='black', lw=1)\n",
    "ax.set_aspect('equal')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a55320",
   "metadata": {},
   "source": [
    "#### **Key Insights**\n",
    "- The diamond shape of L1 means it has corners on the axes.\n",
    "- The Residual Sum of the Squares are more likely to first touch a corder (where a coefficient is 0.)\n",
    "- The circle shape of the L2 has no corners, so solutions are shrunk, but rarely exactly zero.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8abdab9",
   "metadata": {},
   "source": [
    "### **1.3.5 The Role of Alpha ($\\alpha$)**\n",
    "\n",
    "Alpha is the **regularization strength** — it controls the trade-off:\n",
    "\n",
    "- $\\alpha = 0$: No penalty → standard linear regression (OLS)\n",
    "- $\\alpha \\to \\infty$: Maximum penalty → all coefficients shrink to zero\n",
    "- \"Just right\" $\\alpha$: Balances fitting the data vs. keeping coefficients small\n",
    "\n",
    "**Choosing the right alpha is a hyperparameter tuning problem** — we'll cover this systematically in Week 5 with Cross-Validation!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716f8813",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **1.4 Feature Scaling — A CRITICAL Prerequisite**\n",
    "\n",
    "### **1.4.1 Back to Our Traveler Analogy One More Time**\n",
    "\n",
    "Feature scaling is a critical prerequisite because regularization is a \"fairness\" tax based on the size of the coefficient ($w$), and that size is directly dictated by the scale of the feature. In our traveler analogy, imagine if the airline charges based on the **number of items**, not the weight. The travelor would be penalized the same for a heavy bowling ball as for a single sock. This is analogous to using unscaled data with regularization. The penalties are unfair. By scaling data, the traveler is effectively putting everything into \"standard boxes.\" Now, when the airline charges the traveler, it's a fair comparison of how much \"space\" each item is actually using.\n",
    "\n",
    "### **1.4.2 Why Scaling Matters for Regularization**\n",
    "\n",
    "Regularization penalizes the **magnitude** of coefficients. If features are on different scales, the penalty is unfair. Imagine you are predicting house prices using two features:\n",
    "\n",
    "- **Feature A:** Total Square Footage (Scale: 500 to 5,000)\n",
    "- **Feature B:** Number of Bedrooms (Scale: 1 to 5)\n",
    "\n",
    "Because the numbers in Square Footage are much larger, the model only needs a **tiny coefficient** ($w_A$) to have a huge impact on the price. Conversely, it needs a **huge coefficient** ($w_B$) for Bedrooms to make a dent.\n",
    "\n",
    "| Feature | Scale | Required Weight (w) to move price $10k |\n",
    "|---------|-------|----------------------------------------|\n",
    "| Square Feet | 500 – 5000 | $w_A = 2$ |\n",
    "| Bedrooms | 1 – 5 | $w_B = 2500$ |\n",
    "\n",
    "The regularization penalty would disproportionately target the large-coefficient features, even if they're not actually more important.\n",
    "\n",
    "**Solution: Standardize features before regularization!**\n",
    "\n",
    "This can be done by either the **`StandardScaler`**:\n",
    "\n",
    "$$x_{scaled} = \\frac{x - \\mu}{\\sigma}$$\n",
    "\n",
    "After scaling, all features have mean = 0 and standard deviation = 1.\n",
    "\n",
    "Or the **`MinMaxScaler`**:\n",
    "\n",
    "$$x_{scaled} = \\frac{x - x_{min}}{x_{max} - x_{min}}$$\n",
    "\n",
    "After scaling, all features are between $[0,1]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c9140f",
   "metadata": {},
   "source": [
    "### **1.4.3 `StandardScaler` Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c4b554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# IMPORTANT: Fit on training data ONLY, then transform both train and test\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)  # Use training mean/std!\n",
    "\n",
    "# Convert back to DataFrames for readability\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns, index=X_test.index)\n",
    "\n",
    "print(\"Scaled training data statistics:\")\n",
    "print(X_train_scaled.describe().round(2).loc[['mean', 'std']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc69fd82",
   "metadata": {},
   "source": [
    "#### **REMEMBER:** Always fit the scale on TRAINING data only! Otherwise you leak information from the test set (data leakage)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200acf9b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **1.5 Ridge Regression (L2) in Action**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39460016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Regression with different alpha values\n",
    "alphas = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "\n",
    "print(f\"{'Alpha':<10} {'Train MSE':<12} {'Test MSE':<12} {'Test R²':<10} {'# Non-zero Coefs':<18}\")\n",
    "print(\"-\" * 62)\n",
    "\n",
    "ridge_coefs = {}  # Store coefficients for visualization\n",
    "\n",
    "for alpha in alphas:\n",
    "    ridge = Ridge(alpha=alpha)\n",
    "    ridge.fit(X_train_scaled, y_train)\n",
    "\n",
    "    y_train_pred = ridge.predict(X_train_scaled)\n",
    "    y_test_pred = ridge.predict(X_test_scaled)\n",
    "\n",
    "    train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "    test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    n_nonzero = np.sum(np.abs(ridge.coef_) > 1e-6)\n",
    "\n",
    "    ridge_coefs[alpha] = ridge.coef_\n",
    "\n",
    "    print(f\"{alpha:<10} {train_mse:<12.4f} {test_mse:<12.4f} {test_r2:<10.4f} {n_nonzero:<18}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3a52e7",
   "metadata": {},
   "source": [
    "#### **NOTICE**: Ridge SHRINKS coefficients but does not set them to exactly zero!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1196408b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **1.6: Lasso Regression (L1) in Action**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c547ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso Regression with different alpha values\n",
    "lasso_alphas = [0.0001, 0.001, 0.01, 0.1, 0.5, 1.0]\n",
    "\n",
    "print(f\"{'Alpha':<10} {'Train MSE':<12} {'Test MSE':<12} {'Test R²':<10} {'# Non-zero Coefs':<18} {'Zeroed Features'}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "lasso_coefs = {}\n",
    "\n",
    "for alpha in lasso_alphas:\n",
    "    lasso = Lasso(alpha=alpha, max_iter=10000)\n",
    "    lasso.fit(X_train_scaled, y_train)\n",
    "\n",
    "    y_train_pred = lasso.predict(X_train_scaled)\n",
    "    y_test_pred = lasso.predict(X_test_scaled)\n",
    "\n",
    "    train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "    test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "    nonzero_mask = np.abs(lasso.coef_) > 1e-6\n",
    "    n_nonzero = np.sum(nonzero_mask)\n",
    "    zeroed = [f for f, m in zip(X.columns, ~nonzero_mask) if m]\n",
    "\n",
    "    lasso_coefs[alpha] = lasso.coef_\n",
    "\n",
    "    print(f\"{alpha:<10} {train_mse:<12.4f} {test_mse:<12.4f} {test_r2:<10.4f} {n_nonzero:<18} {zeroed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01aa15f",
   "metadata": {},
   "source": [
    "#### **NOTICE**: Lasso DOES set coefficients to exactly zero — it performs FEATURE SELECTION!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd817381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize how Lasso coefficients change with alpha\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "for i, feature in enumerate(X.columns):\n",
    "    coefs = [lasso_coefs[a][i] for a in lasso_alphas]\n",
    "    ax.plot(lasso_alphas, coefs, 'o-', label=feature, linewidth=2)\n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('Alpha (Regularization Strength)', fontsize=13)\n",
    "ax.set_ylabel('Coefficient Value', fontsize=13)\n",
    "ax.set_title('Lasso Regression: Coefficient Paths\\n(Coefficients hit zero as alpha increases — automatic feature selection!)', fontsize=14)\n",
    "ax.axhline(y=0, color='black', linewidth=0.8, linestyle='--')\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd0cdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare OLS, Ridge, and Lasso at a reasonable alpha\n",
    "alpha_compare = 1.0\n",
    "\n",
    "# Fit all three models on scaled data\n",
    "lr_scaled = LinearRegression().fit(X_train_scaled, y_train)\n",
    "ridge_compare = Ridge(alpha=alpha_compare).fit(X_train_scaled, y_train)\n",
    "lasso_compare = Lasso(alpha=0.1, max_iter=10000).fit(X_train_scaled, y_train)\n",
    "\n",
    "# Build comparison table\n",
    "comparison = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'OLS': lr_scaled.coef_,\n",
    "    f'Ridge (α={alpha_compare})': ridge_compare.coef_,\n",
    "    f'Lasso (α=0.1)': lasso_compare.coef_\n",
    "})\n",
    "\n",
    "print(\"Coefficient Comparison (Scaled Features):\")\n",
    "print(\"=\" * 65)\n",
    "print(comparison.to_string(index=False, float_format='{:.4f}'.format))\n",
    "\n",
    "print(f\"\\nTest R² — OLS:   {r2_score(y_test, lr_scaled.predict(X_test_scaled)):.4f}\")\n",
    "print(f\"Test R² — Ridge: {r2_score(y_test, ridge_compare.predict(X_test_scaled)):.4f}\")\n",
    "print(f\"Test R² — Lasso: {r2_score(y_test, lasso_compare.predict(X_test_scaled)):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbb98f8",
   "metadata": {},
   "source": [
    "### **Side-by-Side Comparison of Coefficients**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e210bbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual comparison of coefficients\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x_pos = np.arange(len(X.columns))\n",
    "width = 0.25\n",
    "\n",
    "ax.bar(x_pos - width, lr_scaled.coef_, width, label='OLS (No Regularization)', color='#3498db', alpha=0.8)\n",
    "ax.bar(x_pos, ridge_compare.coef_, width, label=f'Ridge (α={alpha_compare})', color='#2ecc71', alpha=0.8)\n",
    "ax.bar(x_pos + width, lasso_compare.coef_, width, label='Lasso (α=0.1)', color='#e74c3c', alpha=0.8)\n",
    "\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(X.columns, rotation=45, ha='right')\n",
    "ax.set_ylabel('Coefficient Value (Scaled Features)')\n",
    "ax.set_title('OLS vs. Ridge vs. Lasso: Coefficient Comparison')\n",
    "ax.axhline(y=0, color='black', linewidth=0.8)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07572a18",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **1.7 Summary: When to Use What?**\n",
    "\n",
    "| Scenario | Recommended Method | Why |\n",
    "|----------|-------------------|-----|\n",
    "| Few features, all seem important | **Standard OLS** | No need to penalize |\n",
    "| Many features, most seem useful | **Ridge (L2)** | Shrinks all, keeps all |\n",
    "| Many features, want to find the important ones | **Lasso (L1)** | Automatic feature selection |\n",
    "| Many features, some correlated | **Ridge (L2)** | Handles multicollinearity better |\n",
    "| Need a sparse/interpretable model | **Lasso (L1)** | Zeros out irrelevant features |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e701518f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **BREAK (10-15 minutes)**\n",
    "\n",
    "---\n",
    "\n",
    "## **2.1 Lab Exercises** (new notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4b7a1a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **3.1: Review & Wrap-Up**\n",
    "\n",
    "### **3.1.1 What We Learned Today**\n",
    "| Concept | Key Idea |\n",
    "|---------|----------|\n",
    "| Multiple Linear Regression | $\\hat{y} = w_0 + w_1 x_1 + \\cdots + w_p x_p$ — uses all features |\n",
    "| Regularization | Add a penalty to prevent overfitting: $\\text{MSE} + \\alpha \\cdot \\text{Penalty}$ |\n",
    "| Ridge (L2) | Penalty = $\\alpha \\sum w_j^2$ — shrinks, never zeros |\n",
    "| Lasso (L1) | Penalty = $\\alpha \\sum |w_j|$ — shrinks AND can zero out |\n",
    "| Feature Scaling | **Required** before regularization — `StandardScaler` |\n",
    "| Alpha (α) | Controls regularization strength — higher = more penalty |\n",
    "\n",
    "\n",
    "### **3.1.2 Discussion Questions**\n",
    "\n",
    "1. **The Cost of Complexity:** In our traveler analogy, how does the \"baggage fee\" ($\\alpha$) act as a proxy for model complexity? What happens to the model when $\\alpha = 0$ versus when $\\alpha$ is extremely large?\n",
    "2. **The Scaling Problem:** Why is it misleading to compare the \"importance\" of features based solely on their coefficient magnitudes if the features haven't been scaled?\n",
    "3. **Geometry of Sparsity:** Based on the visualization of the L1 diamond and the L2 circle, why is the RSS \"error ellipse\" more likely to hit a corner in Lasso than in Ridge? What does \"hitting a corner\" mean for your final model?\n",
    "4. **Model Selection:** If you are working with a dataset where you expect many features to be irrelevant noise, which regularization method would you choose to perform automatic feature selection?\n",
    "5. **The Trade-off:** Regularization intentionally introduces a small amount of **bias** into the model. What is the benefit we receive in exchange for this added bias?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84824fa6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **3.2 Coming Up**\n",
    "\n",
    "### **3.2.1 Next Week Preview: Cross-Validation & Model Selection**\n",
    "- Choose the best alpha without using the test set\n",
    "- Compare models fairly\n",
    "- Use `GridSearchCV` and `RandomizedSearchCV`\n",
    "- Build complete `Pipeline` objects\n",
    "\n",
    "### **3.2.2 Homework Reminder**\n",
    "- **Lab due:** Monday, 23 February @ 6:00 PM (grace period: Wednesda, 25 February @ 11:59 PM)\n",
    "- **Next week's mastery assessment** will cover tonight's material (Regularization, Ridge, Lasso)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py314",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
